{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Network\n",
    "\n",
    "In this task, we implement a conventional RNN cell and a GRU to understand RNNs. Then we configurate GRU in special ways such that it either recovers a conventional RNN or keeps its memory in long term.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.set_autosave_interval(180000)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosaving every 180 seconds\n"
     ]
    }
   ],
   "source": [
    "# As usual, a bit of setup\n",
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#need to have these two lines to work on my 1060 3gb\n",
    "#  https://stackoverflow.com/questions/43990046/tensorflow-blas-gemm-launch-failed\n",
    "physical_devices = tf.config.list_physical_devices('GPU') \n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%autosave 180\n",
    "\n",
    "def rel_error(x, y):\n",
    "    \"\"\" returns relative error \"\"\"\n",
    "    return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent Neural Networks\n",
    "\n",
    "In this task, you will need to implement forward calculation of recurrent neural networks. Let's first initialize a problem for RNNs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 5, 3)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Setup an example. Provide sizes and the input data. \n",
    "\n",
    "# set sizes \n",
    "time_steps = 5\n",
    "batch_size = 4\n",
    "input_size = 3\n",
    "hidden_size = 2\n",
    "\n",
    "# create input data with shape [batch_size, time_steps, num_features]\n",
    "np.random.seed(137)\n",
    "input_data = np.random.rand(batch_size, time_steps, input_size).astype(np.float32)\n",
    "\n",
    "np.shape(input_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement an RNN and a GRU with tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer gru will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "tf.Tensor(\n",
      "[[ 0.22436616  0.41891837]\n",
      " [ 0.22912285  0.33538428]\n",
      " [ 0.0117369   0.4009116 ]\n",
      " [-0.05511545  0.52123225]], shape=(4, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "## Create RNN layers\n",
    "\n",
    "# tf.random.set_seed(137)\n",
    "\n",
    "# initialize a state of zero for both RNN and GRU\n",
    "# 'state' is a tensor of shape [batch_size, hidden_size]\n",
    "initial_state = tf.constant(np.zeros([batch_size, hidden_size]), dtype=tf.float32)\n",
    "\n",
    "# create a SimpleRNN\n",
    "tfrnn = tf.keras.layers.SimpleRNN(hidden_size, return_sequences=True, return_state=True)\n",
    "\n",
    "# 'outputs' is a tensor of shape [batch_size, time_steps, hidden_size]\n",
    "# RNN cell outputs the hidden state directly, so the output at each step is the hidden state at that step\n",
    "# final_state is the last state of the sequence. final_state == outputs[:, -1, :]\n",
    "\n",
    "tfrnn_outputs, tfrnn_final_state = tfrnn(input_data, initial_state=initial_state)\n",
    "\n",
    "# create a GRU RNN\n",
    "tfgru = tf.keras.layers.GRU(hidden_size, return_sequences=True, return_state=True, reset_after=False)\n",
    "\n",
    "# 'outputs' and `final_state` are the same for a GRU.\n",
    "tfgru_outputs, tfgru_final_state = tfgru(input_data, initial_state=initial_state)\n",
    "\n",
    "# print(tfrnn_outputs)\n",
    "print(tfgru_final_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read out parameters from RNN and GRU cells\n",
    "\n",
    "**Q1 (0 points)** Understanding `SimpleRNN` and `GRU` parameters\n",
    "\n",
    "Please read the code and documentation of `get_rnn_params` and `get_gru_params` to see how to read out parameters from these to models. You will need to use these parameters in your own implementations. NO implementation is needed here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.09189415 -0.9957687 ]\n",
      " [ 0.9957687   0.09189421]]\n",
      "[[ 0.51821375 -0.03997016]\n",
      " [-0.38850176 -0.8335629 ]\n",
      " [-0.3316639  -0.50743043]]\n",
      "[0. 0.]\n"
     ]
    }
   ],
   "source": [
    "from rnn_param_helper import get_rnn_params, get_gru_params\n",
    "\n",
    "wt_h, wt_x, bias = get_rnn_params(tfrnn)\n",
    "\n",
    "wtu_h, wtu_x, biasu, wtr_h, wtr_x, biasr, wtc_h, wtc_x, biasc = get_gru_params(tfgru)\n",
    "\n",
    "print(wt_h) #np array 2x2\n",
    "print(wt_x) #3x2\n",
    "print(bias) #1x2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numpy Implementation\n",
    "**Q2 (3 points)** Please implement your own simple RNN. \n",
    "\n",
    "Your implementation needs to match the tensorflow calculation.\n",
    "\n",
    "**Q3 (5 points)** Please implement your own GRU. \n",
    "\n",
    "Your implementation needs to match the tensorflow calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Difference between your RNN implementation and tf RNN 2.6258160270859277e-07\n",
      "Difference between your GRU implementation and tf GRU 1.8048722367128474e-06\n"
     ]
    }
   ],
   "source": [
    "from implementation import rnn,gru\n",
    "\n",
    "# calculation from your own implemenation of a basic RNN\n",
    "nprnn_outputs, nprnn_final_state = rnn(wt_h, wt_x, bias, initial_state, input_data)\n",
    "\n",
    "print(\"Difference between your RNN implementation and tf RNN\", \n",
    "                     rel_error(tfrnn_outputs, nprnn_outputs) + rel_error(tfrnn_final_state, nprnn_final_state))\n",
    "\n",
    "# print(\"tf: \",tfrnn_outputs[0,:,:].numpy())\n",
    "# print(\"np: \",nprnn_outputs[0,:,:])\n",
    "\n",
    "# calculation from your own implemenation of a GRU RNN\n",
    "npgru_outputs, npgru_final_state = gru(wtu_h, wtu_x, biasu, wtr_h, wtr_x, biasr, wtc_h, wtc_x, biasc, initial_state, input_data)\n",
    "\n",
    "print(\"Difference between your GRU implementation and tf GRU\", \n",
    "      rel_error(tfgru_outputs, npgru_outputs) + rel_error(tfgru_final_state, npgru_final_state))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GRU includes RNN as a special case\n",
    "**Q4 (2 points)** Can you assign a special set of parameters to GRU such that its outputs is almost the same as RNN?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flat in:  15\n",
      "flat out:  36\n",
      "hidden_size =  2\n",
      "input size =  3\n",
      "GRU:  [[0. 0.]]\n",
      "RNN:  [[-0.56673313 -0.02633931]]\n",
      "flat in:  4\n",
      "flat out:  2\n",
      "Model: \"functional_47\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_25 (InputLayer)        [(None, 5)]               0         \n",
      "_________________________________________________________________\n",
      "dense_90 (Dense)             (None, 64)                384       \n",
      "_________________________________________________________________\n",
      "batch_normalization_43 (Batc (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "dense_91 (Dense)             (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_92 (Dense)             (None, 2)                 130       \n",
      "=================================================================\n",
      "Total params: 4,930\n",
      "Trainable params: 4,802\n",
      "Non-trainable params: 128\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "263/282 [==========================>...] - ETA: 0s - loss: 0.3319 - mean_squared_error: 0.3319WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0000s vs `on_test_batch_end` time: 0.0010s). Check your callbacks.\n",
      "282/282 [==============================] - 0s 2ms/step - loss: 0.3166 - mean_squared_error: 0.3166 - val_loss: 0.1005 - val_mean_squared_error: 0.1005\n",
      "Epoch 2/50\n",
      "282/282 [==============================] - 0s 1ms/step - loss: 0.0649 - mean_squared_error: 0.0649 - val_loss: 0.0285 - val_mean_squared_error: 0.0285\n",
      "Epoch 3/50\n",
      "282/282 [==============================] - 0s 1ms/step - loss: 0.0450 - mean_squared_error: 0.0450 - val_loss: 0.0183 - val_mean_squared_error: 0.0183\n",
      "Epoch 4/50\n",
      "282/282 [==============================] - 0s 1ms/step - loss: 0.0349 - mean_squared_error: 0.0349 - val_loss: 0.0144 - val_mean_squared_error: 0.0144\n",
      "Epoch 5/50\n",
      "282/282 [==============================] - 0s 2ms/step - loss: 0.0315 - mean_squared_error: 0.0315 - val_loss: 0.0119 - val_mean_squared_error: 0.0119\n",
      "Epoch 6/50\n",
      "282/282 [==============================] - 0s 1ms/step - loss: 0.0328 - mean_squared_error: 0.0328 - val_loss: 0.0104 - val_mean_squared_error: 0.0104\n",
      "Epoch 7/50\n",
      "282/282 [==============================] - 0s 1ms/step - loss: 0.0281 - mean_squared_error: 0.0281 - val_loss: 0.0091 - val_mean_squared_error: 0.0091\n",
      "Epoch 8/50\n",
      "282/282 [==============================] - 0s 1ms/step - loss: 0.0296 - mean_squared_error: 0.0296 - val_loss: 0.0083 - val_mean_squared_error: 0.0083\n",
      "Epoch 9/50\n",
      "282/282 [==============================] - 0s 2ms/step - loss: 0.0267 - mean_squared_error: 0.0267 - val_loss: 0.0075 - val_mean_squared_error: 0.0075\n",
      "Epoch 10/50\n",
      "282/282 [==============================] - 0s 1ms/step - loss: 0.0269 - mean_squared_error: 0.0269 - val_loss: 0.0068 - val_mean_squared_error: 0.0068\n",
      "Epoch 11/50\n",
      "282/282 [==============================] - 0s 1ms/step - loss: 0.0272 - mean_squared_error: 0.0272 - val_loss: 0.0060 - val_mean_squared_error: 0.0060\n",
      "Epoch 12/50\n",
      "282/282 [==============================] - 0s 1ms/step - loss: 0.0249 - mean_squared_error: 0.0249 - val_loss: 0.0056 - val_mean_squared_error: 0.0056\n",
      "Epoch 13/50\n",
      "282/282 [==============================] - 0s 1ms/step - loss: 0.0228 - mean_squared_error: 0.0228 - val_loss: 0.0052 - val_mean_squared_error: 0.0052\n",
      "Epoch 14/50\n",
      "282/282 [==============================] - 0s 1ms/step - loss: 0.0245 - mean_squared_error: 0.0245 - val_loss: 0.0048 - val_mean_squared_error: 0.0048\n",
      "Epoch 15/50\n",
      "282/282 [==============================] - 0s 1ms/step - loss: 0.0214 - mean_squared_error: 0.0214 - val_loss: 0.0045 - val_mean_squared_error: 0.0045\n",
      "Epoch 16/50\n",
      "282/282 [==============================] - 0s 1ms/step - loss: 0.0211 - mean_squared_error: 0.0211 - val_loss: 0.0045 - val_mean_squared_error: 0.0045\n",
      "Epoch 17/50\n",
      "282/282 [==============================] - 0s 1ms/step - loss: 0.0207 - mean_squared_error: 0.0207 - val_loss: 0.0040 - val_mean_squared_error: 0.0040\n",
      "Epoch 18/50\n",
      "282/282 [==============================] - 0s 1ms/step - loss: 0.0207 - mean_squared_error: 0.0207 - val_loss: 0.0040 - val_mean_squared_error: 0.0040\n",
      "Epoch 19/50\n",
      "282/282 [==============================] - 0s 1ms/step - loss: 0.0189 - mean_squared_error: 0.0189 - val_loss: 0.0042 - val_mean_squared_error: 0.0042\n",
      "Epoch 20/50\n",
      "282/282 [==============================] - 0s 1ms/step - loss: 0.0203 - mean_squared_error: 0.0203 - val_loss: 0.0039 - val_mean_squared_error: 0.0039\n",
      "Epoch 21/50\n",
      "282/282 [==============================] - 0s 1ms/step - loss: 0.0204 - mean_squared_error: 0.0204 - val_loss: 0.0035 - val_mean_squared_error: 0.0035\n",
      "Epoch 22/50\n",
      "282/282 [==============================] - 0s 1ms/step - loss: 0.0187 - mean_squared_error: 0.0187 - val_loss: 0.0038 - val_mean_squared_error: 0.0038\n",
      "Epoch 23/50\n",
      "282/282 [==============================] - 0s 1ms/step - loss: 0.0189 - mean_squared_error: 0.0189 - val_loss: 0.0033 - val_mean_squared_error: 0.0033\n",
      "Epoch 24/50\n",
      "282/282 [==============================] - 0s 1ms/step - loss: 0.0179 - mean_squared_error: 0.0179 - val_loss: 0.0032 - val_mean_squared_error: 0.0032\n",
      "Epoch 25/50\n",
      "282/282 [==============================] - 0s 1ms/step - loss: 0.0183 - mean_squared_error: 0.0183 - val_loss: 0.0033 - val_mean_squared_error: 0.0033\n",
      "Epoch 26/50\n",
      "282/282 [==============================] - 0s 1ms/step - loss: 0.0174 - mean_squared_error: 0.0174 - val_loss: 0.0032 - val_mean_squared_error: 0.0032\n",
      "Epoch 27/50\n",
      "282/282 [==============================] - 0s 1ms/step - loss: 0.0170 - mean_squared_error: 0.0170 - val_loss: 0.0030 - val_mean_squared_error: 0.0030\n",
      "Epoch 28/50\n",
      "282/282 [==============================] - 0s 2ms/step - loss: 0.0165 - mean_squared_error: 0.0165 - val_loss: 0.0030 - val_mean_squared_error: 0.0030\n",
      "Epoch 29/50\n",
      "282/282 [==============================] - 0s 2ms/step - loss: 0.0186 - mean_squared_error: 0.0186 - val_loss: 0.0030 - val_mean_squared_error: 0.0030\n",
      "Epoch 30/50\n",
      "282/282 [==============================] - 0s 1ms/step - loss: 0.0163 - mean_squared_error: 0.0163 - val_loss: 0.0031 - val_mean_squared_error: 0.0031\n",
      "Epoch 31/50\n",
      "282/282 [==============================] - 0s 1ms/step - loss: 0.0164 - mean_squared_error: 0.0164 - val_loss: 0.0029 - val_mean_squared_error: 0.0029\n",
      "Epoch 32/50\n",
      "282/282 [==============================] - 0s 1ms/step - loss: 0.0156 - mean_squared_error: 0.0156 - val_loss: 0.0029 - val_mean_squared_error: 0.0029\n",
      "Epoch 33/50\n",
      "282/282 [==============================] - 0s 1ms/step - loss: 0.0157 - mean_squared_error: 0.0157 - val_loss: 0.0029 - val_mean_squared_error: 0.0029\n",
      "Epoch 34/50\n",
      "282/282 [==============================] - 0s 1ms/step - loss: 0.0146 - mean_squared_error: 0.0146 - val_loss: 0.0026 - val_mean_squared_error: 0.0026\n",
      "Epoch 35/50\n",
      "282/282 [==============================] - 0s 1ms/step - loss: 0.0147 - mean_squared_error: 0.0147 - val_loss: 0.0026 - val_mean_squared_error: 0.0026\n",
      "Epoch 36/50\n",
      "282/282 [==============================] - 0s 1ms/step - loss: 0.0151 - mean_squared_error: 0.0151 - val_loss: 0.0024 - val_mean_squared_error: 0.0024\n",
      "Epoch 37/50\n",
      "282/282 [==============================] - 0s 1ms/step - loss: 0.0145 - mean_squared_error: 0.0145 - val_loss: 0.0024 - val_mean_squared_error: 0.0024\n",
      "Epoch 38/50\n",
      "282/282 [==============================] - 0s 1ms/step - loss: 0.0136 - mean_squared_error: 0.0136 - val_loss: 0.0025 - val_mean_squared_error: 0.0025\n",
      "Epoch 39/50\n",
      "282/282 [==============================] - 0s 1ms/step - loss: 0.0140 - mean_squared_error: 0.0140 - val_loss: 0.0024 - val_mean_squared_error: 0.0024\n",
      "Epoch 40/50\n",
      "282/282 [==============================] - 0s 1ms/step - loss: 0.0140 - mean_squared_error: 0.0140 - val_loss: 0.0025 - val_mean_squared_error: 0.0025\n",
      "Epoch 41/50\n",
      "282/282 [==============================] - 0s 1ms/step - loss: 0.0136 - mean_squared_error: 0.0136 - val_loss: 0.0022 - val_mean_squared_error: 0.0022\n",
      "Epoch 42/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "282/282 [==============================] - 0s 1ms/step - loss: 0.0147 - mean_squared_error: 0.0147 - val_loss: 0.0025 - val_mean_squared_error: 0.0025\n",
      "Epoch 43/50\n",
      "282/282 [==============================] - 0s 1ms/step - loss: 0.0143 - mean_squared_error: 0.0143 - val_loss: 0.0022 - val_mean_squared_error: 0.0022\n",
      "Epoch 44/50\n",
      "282/282 [==============================] - 0s 1ms/step - loss: 0.0130 - mean_squared_error: 0.0130 - val_loss: 0.0021 - val_mean_squared_error: 0.0021\n",
      "Epoch 45/50\n",
      "282/282 [==============================] - 0s 1ms/step - loss: 0.0128 - mean_squared_error: 0.0128 - val_loss: 0.0022 - val_mean_squared_error: 0.0022\n",
      "Epoch 46/50\n",
      "282/282 [==============================] - 0s 1ms/step - loss: 0.0135 - mean_squared_error: 0.0135 - val_loss: 0.0022 - val_mean_squared_error: 0.0022\n",
      "Epoch 47/50\n",
      "282/282 [==============================] - 0s 1ms/step - loss: 0.0130 - mean_squared_error: 0.0130 - val_loss: 0.0026 - val_mean_squared_error: 0.0026\n",
      "Epoch 48/50\n",
      "282/282 [==============================] - 0s 1ms/step - loss: 0.0126 - mean_squared_error: 0.0126 - val_loss: 0.0022 - val_mean_squared_error: 0.0022\n",
      "Epoch 49/50\n",
      "282/282 [==============================] - 0s 1ms/step - loss: 0.0129 - mean_squared_error: 0.0129 - val_loss: 0.0020 - val_mean_squared_error: 0.0020\n",
      "Epoch 50/50\n",
      "282/282 [==============================] - 0s 1ms/step - loss: 0.0124 - mean_squared_error: 0.0124 - val_loss: 0.0021 - val_mean_squared_error: 0.0021\n",
      "WARNING:tensorflow:Layer gru_22 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "Difference between RNN and a special GRU 1.0\n"
     ]
    }
   ],
   "source": [
    "# Assign some value to a parameter of GRU\n",
    "\n",
    "from implementation import init_gru_with_rnn\n",
    "\n",
    "flat_in = input_size + np.shape(wt_x)[0]*np.shape(wt_x)[1] + np.shape(wt_h)[0]*np.shape(wt_h)[1] + np.shape(bias)[0]\n",
    "print(\"flat in: \",flat_in)\n",
    "flat_out = 3*hidden_size*hidden_size + 3*input_size*hidden_size + 3*hidden_size\n",
    "print(\"flat out: \",flat_out)\n",
    "\n",
    "wtz_h0, wtz_x0, biasz0, wtr_h0, wtr_x0, biasr0, wtc_h0, wtc_x0, biasc0 = init_gru_with_rnn(wt_h, wt_x, bias)\n",
    "\n",
    "# concatenate these parameters to initialize GRU kernels\n",
    "kernel_init = np.concatenate([wtz_x0, wtr_x0, wtc_x0], axis=1)\n",
    "rec_kernel_init = np.concatenate([wtz_h0, wtr_h0, wtc_h0], axis=1)\n",
    "bias_init = np.concatenate([biasz0, biasr0, biasc0], axis=0)\n",
    "\n",
    "grurnn = tf.keras.layers.GRU(hidden_size, \n",
    "                            kernel_initializer=tf.keras.initializers.Constant(kernel_init),\n",
    "                            recurrent_initializer=tf.keras.initializers.Constant(rec_kernel_init), \n",
    "                            bias_initializer=tf.keras.initializers.Constant(bias_init),\n",
    "                            return_sequences=True, return_state=True, reset_after=False)\n",
    "\n",
    "# 'outputs' is a tensor of shape [batch_size, time_steps, hidden_size]\n",
    "# Same as the basic RNN cell, final_state == outputs[:, -1, :]\n",
    "grurnn_outputs, grurnn_final_state = grurnn(input_data, initial_state=initial_state)\n",
    "\n",
    "\n",
    "# they are the same as the calculation from the basic RNN\n",
    "print(\"Difference between RNN and a special GRU\", rel_error(tfrnn_outputs, grurnn_outputs))\n",
    "\n",
    "# print(\"tf: \", tfrnn_outputs)\n",
    "# print(\"np: \", grurnn_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Long-term dependency in RNNs\n",
    "\n",
    "### Long-term dependency in conventional RNNs\n",
    "\n",
    "In this experiment, we check whether an RNN has long-term memory. Particularly, we initialize the start state with different values. If a later hidden state is the same no matter which initialization is used, then it means that the RNN does not carry the information of the initial state to that hidden state. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a larger problem\n",
    "\n",
    "# set sizes \n",
    "time_steps = 50\n",
    "batch_size = 100\n",
    "input_size = 5\n",
    "hidden_size = 8\n",
    "\n",
    "# create input data with shape [batch_size, time_steps, num_features]\n",
    "np.random.seed(137)\n",
    "tf.random.set_seed(137)\n",
    "\n",
    "    \n",
    "# set values for the basic RNN model\n",
    "\n",
    "# TODO: play with the scale, and see if you can find any value that achieves long-term memory\n",
    "scale = 1.0\n",
    "wt_h2 = ((np.random.rand(hidden_size, hidden_size) - 0.5) * scale).astype(np.float32)\n",
    "wt_x2 = ((np.random.rand(input_size, hidden_size) - 0.5) * scale).astype(np.float32)\n",
    "bias2 = ((np.random.rand(hidden_size) - 0.5) * scale).astype(np.float32)\n",
    "\n",
    "rnn2 = tf.keras.layers.SimpleRNN(hidden_size, \n",
    "                                 kernel_initializer=tf.keras.initializers.Constant(wt_x2),\n",
    "                                 recurrent_initializer=tf.keras.initializers.Constant(wt_h2), \n",
    "                                 bias_initializer=tf.keras.initializers.Constant(bias2),\n",
    "                                 return_sequences=True, return_state=False)\n",
    "\n",
    "\n",
    "input_data = (np.random.rand(batch_size, time_steps, input_size).astype(np.float32) - 0.5).astype(np.float32)\n",
    "\n",
    "# intialize the model with different initial states and then calculate the final state\n",
    "init_states = [np.zeros([batch_size, hidden_size], dtype=np.float32), \n",
    "               np.random.random_sample([batch_size, hidden_size]).astype(np.float32), \n",
    "               np.random.random_sample([batch_size, hidden_size]).astype(np.float32)]\n",
    "init_states = init_states + [init_states[1] * 100]\n",
    "\n",
    "\n",
    "def show_hist_of_hidden_values(outputs, step, title, do_plot=True):\n",
    "    \"\"\" \n",
    "    Compute differences of outputs at time step `step` with different initial states. If \n",
    "    differences are mostly zero, then it means the initial state does not have effect to the \n",
    "    output, which means the memory about the initial state is lost.\n",
    "    \"\"\"\n",
    "\n",
    "    # plot the difference between the four difference settings\n",
    "    diff_list = []\n",
    "    \n",
    "    for i in range(len(outputs) - 1):\n",
    "        for j in range(i + 1, len(outputs)):\n",
    "            diff = np.linalg.norm(outputs[i][:, step, :] - outputs[j][:, step, :], axis=1)\n",
    "            \n",
    "            diff_list.append(diff)\n",
    "\n",
    "    diff_frac = 0.0\n",
    "    for k in range(len(diff_list)):\n",
    "        diff_frac = diff_frac + np.mean((diff_list[k] > 0.1).astype(np.float32))\n",
    "\n",
    "    diff_frac = diff_frac / len(diff_list)\n",
    "\n",
    "    if do_plot:\n",
    "        # plot the histogram of norms of differences\n",
    "        n_bins = 20\n",
    "        fig, axs = plt.subplots(2, 3, sharey=True, tight_layout=True)\n",
    "        plt.suptitle(title, fontsize=16)\n",
    "    \n",
    "        for k in range(len(diff_list)):\n",
    "            axs[k // 3,  k % 3].hist(diff_list[k], bins=n_bins)\n",
    "    \n",
    "    return diff_frac\n",
    "\n",
    "#######################################################################################################\n",
    "\n",
    "outputs = [None] * len(init_states)\n",
    "\n",
    "for i in range(len(init_states)): \n",
    "    outputs[i] = rnn2(tf.constant(input_data), initial_state = tf.constant(init_states[i]))\n",
    "    outputs[i] = outputs[i].numpy()\n",
    "    \n",
    "    \n",
    "diff_frac5 = show_hist_of_hidden_values(outputs, 5, \n",
    "                           'Histogram of differences at step 5 with different initializations')\n",
    "\n",
    "diff_frac10 = show_hist_of_hidden_values(outputs, 10, \n",
    "                           'Histogram of differences at step 10 with different initializations')\n",
    "\n",
    "diff_frac20 = show_hist_of_hidden_values(outputs, 20, \n",
    "                           'Histogram of differences at step 20 with different initializations')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Fractions of differences above 0.1 at step 5, 10, and 20:')\n",
    "print('%.3f, %.3f, %.3f' % (diff_frac5, diff_frac10, diff_frac20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q5(3 points)** Interpret the plots above in terms of long-term dependency. (Answers like \"These plots indicates the conclusion that ...\" will get zero points. You need to clearly indicate which part of the plot is supporting your conclusion and why.) \n",
    "\n",
    "*Answer:*\n",
    "The data becomes more and more skewed to the right as the number of steps increases. For example, at step 5 the data appears to be somewhat evenly distributed, however, by stem 20 _____ This indicates that ___\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Long-term dependency in GRUs\n",
    "\n",
    "**Q6 (2 points)** Can you set GRU parameters such that it maintains the initial state in the memory for a long term? You can see the histogram of differences, and your code will be graded by the function below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from implementation import init_gru_with_long_term_memory\n",
    "\n",
    "wtz_h, wtz_x, biasz, wtr_h, wtr_x, biasr, wth_h, wth_x, biash = init_gru_with_long_term_memory(wt_h2, wt_x2, bias2)\n",
    "\n",
    "# concatenate these parameters to initialize GRU kernels\n",
    "kernel_init = np.concatenate([wtz_x, wtr_x, wth_x], axis=1)\n",
    "rec_kernel_init = np.concatenate([wtz_h, wtr_h, wth_h], axis=1)\n",
    "bias_init = np.concatenate([biasz, biasr, biash], axis=0)\n",
    "\n",
    "gru2 = tf.keras.layers.GRU(hidden_size, \n",
    "                            kernel_initializer=tf.keras.initializers.Constant(kernel_init),\n",
    "                            recurrent_initializer=tf.keras.initializers.Constant(rec_kernel_init), \n",
    "                            bias_initializer=tf.keras.initializers.Constant(bias_init),\n",
    "                            return_sequences=True, return_state=False, reset_after=False)\n",
    "\n",
    "\n",
    "\n",
    "outputs = [None] * len(init_states)\n",
    "\n",
    "for i in range(len(init_states)): \n",
    "    outputs[i] = gru2(tf.constant(input_data), initial_state = tf.constant(init_states[i]))\n",
    "    outputs[i] = outputs[i].numpy()\n",
    "    \n",
    "    print('Difference between a later hidden state and the initial state is', np.mean(np.abs(outputs[i][:, 20, :] - init_states[i])))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation: vanishing gradients and exploding gradients\n",
    "\n",
    "### Conventional RNN\n",
    "In the experiment, you will observe vanishing gradients and exploding gradients (mostly vanishing gradients) from a conventional RNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate gradient with respect to the initial state\n",
    "\n",
    "# TODO: play with the random seed and the scale and see how likely the gradients are near zero\n",
    "np.random.seed(2020)\n",
    "scale = 1.0\n",
    "wt_h3 = ((np.random.rand(hidden_size, hidden_size) - 0.5) * scale).astype(np.float32)\n",
    "wt_x3 = ((np.random.rand(input_size, hidden_size) - 0.5) * scale).astype(np.float32)\n",
    "bias3 = ((np.random.rand(hidden_size) - 0.5) * scale).astype(np.float32)\n",
    "\n",
    "rnn3 = tf.keras.layers.SimpleRNN(hidden_size, \n",
    "                                 kernel_initializer=tf.keras.initializers.Constant(wt_x3),\n",
    "                                 recurrent_initializer=tf.keras.initializers.Constant(wt_h3), \n",
    "                                 bias_initializer=tf.keras.initializers.Constant(bias3),\n",
    "                                 return_sequences=True, return_state=False)\n",
    "\n",
    "\n",
    "init_state = tf.Variable(np.random.random_sample([batch_size, hidden_size]), dtype=tf.float32)\n",
    "\n",
    "with tf.GradientTape(persistent=True) as g:\n",
    "    g.watch(init_state)\n",
    "    outputs = rnn3(input_data, initial_state=init_state)\n",
    "    obj = tf.reduce_sum(tf.square(outputs[:, 30, :]))\n",
    "    \n",
    "d_init_state = g.gradient(obj, init_state) \n",
    "\n",
    "# show the norms of gradients. Most of them are zero. \n",
    "rnn_grad_norm = np.linalg.norm(d_init_state, axis=1)\n",
    "\n",
    "n_bins = 20\n",
    "_ = plt.hist(rnn_grad_norm, bins=n_bins)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GRU\n",
    "\n",
    "If a GRU is configurated in a way such that it holds its memory, then there is no gradient vanishing problem. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# set up a GRU with long-term memory\n",
    "wtz_h, wtz_x, biasz, wtr_h, wtr_x, biasr, wth_h, wth_x, biash = init_gru_with_long_term_memory(wt_h3, wt_x3, bias3)\n",
    "\n",
    "# concatenate these parameters to initialize GRU kernels\n",
    "kernel_init = np.concatenate([wtz_x, wtr_x, wth_x], axis=1)\n",
    "rec_kernel_init = np.concatenate([wtz_h, wtr_h, wth_h], axis=1)\n",
    "bias_init = np.concatenate([biasz, biasr, biash], axis=0)\n",
    "\n",
    "gru3 = tf.keras.layers.GRU(hidden_size, \n",
    "                            kernel_initializer=tf.keras.initializers.Constant(kernel_init),\n",
    "                            recurrent_initializer=tf.keras.initializers.Constant(rec_kernel_init), \n",
    "                            bias_initializer=tf.keras.initializers.Constant(bias_init),\n",
    "                            return_sequences=True, return_state=False, reset_after=False)\n",
    "\n",
    "\n",
    "\n",
    "# Feed in an initial state to the GRU and compute the gradient\n",
    "init_state = tf.Variable(np.random.random_sample([batch_size, hidden_size]), dtype=tf.float32)\n",
    "with tf.GradientTape(persistent=True) as g:\n",
    "    g.watch(init_state)\n",
    "    outputs = gru3(input_data, initial_state=init_state)\n",
    "    obj = tf.reduce_sum(tf.square(outputs[:, 30, :]))\n",
    "    \n",
    "d_init_state = g.gradient(obj, init_state) \n",
    "\n",
    "# show the norms of gradients. \n",
    "rnn_grad_norm = np.linalg.norm(d_init_state, axis=1)\n",
    "\n",
    "n_bins = 20\n",
    "_ = plt.hist(rnn_grad_norm, bins=n_bins)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Network\n",
    "\n",
    "In this task, we implement a conventional RNN cell and a GRU to understand RNNs. Then we configurate GRU in special ways such that it either recovers a conventional RNN or keeps its memory in long term.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.set_autosave_interval(180000)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosaving every 180 seconds\n"
     ]
    }
   ],
   "source": [
    "# As usual, a bit of setup\n",
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#need to have these two lines to work on my 1060 3gb\n",
    "#  https://stackoverflow.com/questions/43990046/tensorflow-blas-gemm-launch-failed\n",
    "physical_devices = tf.config.list_physical_devices('GPU') \n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%autosave 180\n",
    "\n",
    "def rel_error(x, y):\n",
    "    \"\"\" returns relative error \"\"\"\n",
    "    return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent Neural Networks\n",
    "\n",
    "In this task, you will need to implement forward calculation of recurrent neural networks. Let's first initialize a problem for RNNs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 5, 3)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Setup an example. Provide sizes and the input data. \n",
    "\n",
    "# set sizes \n",
    "time_steps = 5\n",
    "batch_size = 4\n",
    "input_size = 3\n",
    "hidden_size = 2\n",
    "\n",
    "# create input data with shape [batch_size, time_steps, num_features]\n",
    "np.random.seed(137)\n",
    "input_data = np.random.rand(batch_size, time_steps, input_size).astype(np.float32)\n",
    "\n",
    "np.shape(input_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement an RNN and a GRU with tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer gru_1 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "tf.Tensor(\n",
      "[[0.20505044 0.32435158]\n",
      " [0.18454266 0.1305858 ]\n",
      " [0.04225585 0.31837878]\n",
      " [0.0919673  0.2352741 ]], shape=(4, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "## Create RNN layers\n",
    "\n",
    "# tf.random.set_seed(137)\n",
    "\n",
    "# initialize a state of zero for both RNN and GRU\n",
    "# 'state' is a tensor of shape [batch_size, hidden_size]\n",
    "initial_state = tf.constant(np.zeros([batch_size, hidden_size]), dtype=tf.float32)\n",
    "\n",
    "# create a SimpleRNN\n",
    "tfrnn = tf.keras.layers.SimpleRNN(hidden_size, return_sequences=True, return_state=True)\n",
    "\n",
    "# 'outputs' is a tensor of shape [batch_size, time_steps, hidden_size]\n",
    "# RNN cell outputs the hidden state directly, so the output at each step is the hidden state at that step\n",
    "# final_state is the last state of the sequence. final_state == outputs[:, -1, :]\n",
    "\n",
    "tfrnn_outputs, tfrnn_final_state = tfrnn(input_data, initial_state=initial_state)\n",
    "\n",
    "# create a GRU RNN\n",
    "tfgru = tf.keras.layers.GRU(hidden_size, return_sequences=True, return_state=True, reset_after=False)\n",
    "\n",
    "# 'outputs' and `final_state` are the same for a GRU.\n",
    "tfgru_outputs, tfgru_final_state = tfgru(input_data, initial_state=initial_state)\n",
    "\n",
    "# print(tfrnn_outputs)\n",
    "print(tfgru_final_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read out parameters from RNN and GRU cells\n",
    "\n",
    "**Q1 (0 points)** Understanding `SimpleRNN` and `GRU` parameters\n",
    "\n",
    "Please read the code and documentation of `get_rnn_params` and `get_gru_params` to see how to read out parameters from these to models. You will need to use these parameters in your own implementations. NO implementation is needed here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.3171667   0.94836986]\n",
      " [ 0.94836986  0.31716663]]\n",
      "[[ 0.47732222 -1.0932009 ]\n",
      " [ 0.6661321   0.98514676]\n",
      " [ 0.45451462  0.3654003 ]]\n",
      "[0. 0.]\n"
     ]
    }
   ],
   "source": [
    "from rnn_param_helper import get_rnn_params, get_gru_params\n",
    "\n",
    "wt_h, wt_x, bias = get_rnn_params(tfrnn)\n",
    "\n",
    "wtu_h, wtu_x, biasu, wtr_h, wtr_x, biasr, wtc_h, wtc_x, biasc = get_gru_params(tfgru)\n",
    "\n",
    "print(wt_h) #np array 2x2\n",
    "print(wt_x) #3x2\n",
    "print(bias) #1x2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numpy Implementation\n",
    "**Q2 (3 points)** Please implement your own simple RNN. \n",
    "\n",
    "Your implementation needs to match the tensorflow calculation.\n",
    "\n",
    "**Q3 (5 points)** Please implement your own GRU. \n",
    "\n",
    "Your implementation needs to match the tensorflow calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape outputs:  (4, 5, 2)\n",
      "Difference between your RNN implementation and tf RNN 1.9999999227452296\n",
      "tf:  [[ 0.6780842  -0.5983033 ]\n",
      " [ 0.11245049  0.34868872]\n",
      " [ 0.8227119  -0.34554443]\n",
      " [ 0.5324557   0.91269016]\n",
      " [ 0.9539195   0.73356485]]\n",
      "np:  [[ 0.39083016 -0.33215642]\n",
      " [ 0.22433925  0.08774376]\n",
      " [ 0.41455269 -0.16693598]\n",
      " [ 0.41877353  0.541646  ]\n",
      " [ 0.65206611  0.34120297]]\n"
     ]
    }
   ],
   "source": [
    "from implementation import rnn,gru\n",
    "\n",
    "# calculation from your own implemenation of a basic RNN\n",
    "nprnn_outputs, nprnn_final_state = rnn(wt_h, wt_x, bias, initial_state, input_data)\n",
    "\n",
    "# print(\"from TF: \", tfrnn_outputs[0,:,:])\n",
    "# print(\"TF final state: \", tfrnn_final_state.numpy)\n",
    "# print(\"from numpy: \", nprnn_outputs[0,:,:])\n",
    "# print(\"np final state: \", nprnn_final_state)\n",
    "# print(tfrnn_final_state.numpy() - nprnn_final_state)\n",
    "\n",
    "print(\"Difference between your RNN implementation and tf RNN\", \n",
    "                     rel_error(tfrnn_outputs, nprnn_outputs) + rel_error(tfrnn_final_state, nprnn_final_state))\n",
    "\n",
    "#not sure what's going on here - everything is either <0.25 or =1\n",
    "# print((np.abs(tfrnn_outputs - nprnn_outputs) / (np.abs(tfrnn_outputs) + np.abs(nprnn_outputs))))\n",
    "#THIS IS BECAUSE I HAVE ZEROS IN MY OUTPUT\n",
    "\n",
    "print(\"tf: \",tfrnn_outputs[0,:,:].numpy())\n",
    "print(\"np: \",nprnn_outputs[0,:,:])\n",
    "\n",
    "# calculation from your own implemenation of a GRU RNN\n",
    "npgru_outputs, npgru_final_state = gru(wtu_h, wtu_x, biasu, wtr_h, wtr_x, biasr, wtc_h, wtc_x, biasc, initial_state, input_data)\n",
    "\n",
    "# print(\"Difference between your GRU implementation and tf GRU\", \n",
    "#       rel_error(tfgru_outputs, npgru_outputs) + rel_error(tfgru_final_state, npgru_final_state))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GRU includes RNN as a special case\n",
    "**Q4 (2 points)** Can you assign a special set of parameters to GRU such that its outputs is almost the same as RNN?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign some value to a parameter of GRU\n",
    "\n",
    "from implementation import init_gru_with_rnn\n",
    "\n",
    "wtz_h0, wtz_x0, biasz0, wtr_h0, wtr_x0, biasr0, wtc_h0, wtc_x0, biasc0 = init_gru_with_rnn(wt_h, wt_x, bias)\n",
    "\n",
    "# concatenate these parameters to initialize GRU kernels\n",
    "kernel_init = np.concatenate([wtz_x0, wtr_x0, wtc_x0], axis=1)\n",
    "rec_kernel_init = np.concatenate([wtz_h0, wtr_h0, wtc_h0], axis=1)\n",
    "bias_init = np.concatenate([biasz0, biasr0, biasc0], axis=0)\n",
    "\n",
    "grurnn = tf.keras.layers.GRU(hidden_size, \n",
    "                            kernel_initializer=tf.keras.initializers.Constant(kernel_init),\n",
    "                            recurrent_initializer=tf.keras.initializers.Constant(rec_kernel_init), \n",
    "                            bias_initializer=tf.keras.initializers.Constant(bias_init),\n",
    "                            return_sequences=True, return_state=True, reset_after=False)\n",
    "\n",
    "# 'outputs' is a tensor of shape [batch_size, time_steps, hidden_size]\n",
    "# Same as the basic RNN cell, final_state == outputs[:, -1, :]\n",
    "grurnn_outputs, grurnn_final_state = grurnn(input_data, initial_state=initial_state)\n",
    "\n",
    "\n",
    "# they are the same as the calculation from the basic RNN\n",
    "print(\"Difference between RNN and a special GRU\", rel_error(tfrnn_outputs, grurnn_outputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Long-term dependency in RNNs\n",
    "\n",
    "### Long-term dependency in conventional RNNs\n",
    "\n",
    "In this experiment, we check whether an RNN has long-term memory. Particularly, we initialize the start state with different values. If a later hidden state is the same no matter which initialization is used, then it means that the RNN does not carry the information of the initial state to that hidden state. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a larger problem\n",
    "\n",
    "# set sizes \n",
    "time_steps = 50\n",
    "batch_size = 100\n",
    "input_size = 5\n",
    "hidden_size = 8\n",
    "\n",
    "# create input data with shape [batch_size, time_steps, num_features]\n",
    "np.random.seed(137)\n",
    "tf.random.set_seed(137)\n",
    "\n",
    "    \n",
    "# set values for the basic RNN model\n",
    "\n",
    "# TODO: play with the scale, and see if you can find any value that achieves long-term memory\n",
    "scale = 1.0\n",
    "wt_h2 = ((np.random.rand(hidden_size, hidden_size) - 0.5) * scale).astype(np.float32)\n",
    "wt_x2 = ((np.random.rand(input_size, hidden_size) - 0.5) * scale).astype(np.float32)\n",
    "bias2 = ((np.random.rand(hidden_size) - 0.5) * scale).astype(np.float32)\n",
    "\n",
    "rnn2 = tf.keras.layers.SimpleRNN(hidden_size, \n",
    "                                 kernel_initializer=tf.keras.initializers.Constant(wt_x2),\n",
    "                                 recurrent_initializer=tf.keras.initializers.Constant(wt_h2), \n",
    "                                 bias_initializer=tf.keras.initializers.Constant(bias2),\n",
    "                                 return_sequences=True, return_state=False)\n",
    "\n",
    "\n",
    "input_data = (np.random.rand(batch_size, time_steps, input_size).astype(np.float32) - 0.5).astype(np.float32)\n",
    "\n",
    "# intialize the model with different initial states and then calculate the final state\n",
    "init_states = [np.zeros([batch_size, hidden_size], dtype=np.float32), \n",
    "               np.random.random_sample([batch_size, hidden_size]).astype(np.float32), \n",
    "               np.random.random_sample([batch_size, hidden_size]).astype(np.float32)]\n",
    "init_states = init_states + [init_states[1] * 100]\n",
    "\n",
    "\n",
    "def show_hist_of_hidden_values(outputs, step, title, do_plot=True):\n",
    "    \"\"\" \n",
    "    Compute differences of outputs at time step `step` with different initial states. If \n",
    "    differences are mostly zero, then it means the initial state does not have effect to the \n",
    "    output, which means the memory about the initial state is lost.\n",
    "    \"\"\"\n",
    "\n",
    "    # plot the difference between the four difference settings\n",
    "    diff_list = []\n",
    "    \n",
    "    for i in range(len(outputs) - 1):\n",
    "        for j in range(i + 1, len(outputs)):\n",
    "            diff = np.linalg.norm(outputs[i][:, step, :] - outputs[j][:, step, :], axis=1)\n",
    "            \n",
    "            diff_list.append(diff)\n",
    "\n",
    "    diff_frac = 0.0\n",
    "    for k in range(len(diff_list)):\n",
    "        diff_frac = diff_frac + np.mean((diff_list[k] > 0.1).astype(np.float32))\n",
    "\n",
    "    diff_frac = diff_frac / len(diff_list)\n",
    "\n",
    "    if do_plot:\n",
    "        # plot the histogram of norms of differences\n",
    "        n_bins = 20\n",
    "        fig, axs = plt.subplots(2, 3, sharey=True, tight_layout=True)\n",
    "        plt.suptitle(title, fontsize=16)\n",
    "    \n",
    "        for k in range(len(diff_list)):\n",
    "            axs[k // 3,  k % 3].hist(diff_list[k], bins=n_bins)\n",
    "    \n",
    "    return diff_frac\n",
    "\n",
    "#######################################################################################################\n",
    "\n",
    "outputs = [None] * len(init_states)\n",
    "\n",
    "for i in range(len(init_states)): \n",
    "    outputs[i] = rnn2(tf.constant(input_data), initial_state = tf.constant(init_states[i]))\n",
    "    outputs[i] = outputs[i].numpy()\n",
    "    \n",
    "    \n",
    "diff_frac5 = show_hist_of_hidden_values(outputs, 5, \n",
    "                           'Histogram of differences at step 5 with different initializations')\n",
    "\n",
    "diff_frac10 = show_hist_of_hidden_values(outputs, 10, \n",
    "                           'Histogram of differences at step 10 with different initializations')\n",
    "\n",
    "diff_frac20 = show_hist_of_hidden_values(outputs, 20, \n",
    "                           'Histogram of differences at step 20 with different initializations')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Fractions of differences above 0.1 at step 5, 10, and 20:')\n",
    "print('%.3f, %.3f, %.3f' % (diff_frac5, diff_frac10, diff_frac20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q5(3 points)** Interpret the plots above in terms of long-term dependency. (Answers like \"These plots indicates the conclusion that ...\" will get zero points. You need to clearly indicate which part of the plot is supporting your conclusion and why.) \n",
    "\n",
    "*Answer:*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Long-term dependency in GRUs\n",
    "\n",
    "**Q6 (2 points)** Can you set GRU parameters such that it maintains the initial state in the memory for a long term? You can see the histogram of differences, and your code will be graded by the function below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from implementation import init_gru_with_long_term_memory\n",
    "\n",
    "wtz_h, wtz_x, biasz, wtr_h, wtr_x, biasr, wth_h, wth_x, biash = init_gru_with_long_term_memory(wt_h2, wt_x2, bias2)\n",
    "\n",
    "# concatenate these parameters to initialize GRU kernels\n",
    "kernel_init = np.concatenate([wtz_x, wtr_x, wth_x], axis=1)\n",
    "rec_kernel_init = np.concatenate([wtz_h, wtr_h, wth_h], axis=1)\n",
    "bias_init = np.concatenate([biasz, biasr, biash], axis=0)\n",
    "\n",
    "gru2 = tf.keras.layers.GRU(hidden_size, \n",
    "                            kernel_initializer=tf.keras.initializers.Constant(kernel_init),\n",
    "                            recurrent_initializer=tf.keras.initializers.Constant(rec_kernel_init), \n",
    "                            bias_initializer=tf.keras.initializers.Constant(bias_init),\n",
    "                            return_sequences=True, return_state=False, reset_after=False)\n",
    "\n",
    "\n",
    "\n",
    "outputs = [None] * len(init_states)\n",
    "\n",
    "for i in range(len(init_states)): \n",
    "    outputs[i] = gru2(tf.constant(input_data), initial_state = tf.constant(init_states[i]))\n",
    "    outputs[i] = outputs[i].numpy()\n",
    "    \n",
    "    print('Difference between a later hidden state and the initial state is', np.mean(np.abs(outputs[i][:, 20, :] - init_states[i])))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation: vanishing gradients and exploding gradients\n",
    "\n",
    "### Conventional RNN\n",
    "In the experiment, you will observe vanishing gradients and exploding gradients (mostly vanishing gradients) from a conventional RNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate gradient with respect to the initial state\n",
    "\n",
    "# TODO: play with the random seed and the scale and see how likely the gradients are near zero\n",
    "np.random.seed(2020)\n",
    "scale = 1.0\n",
    "wt_h3 = ((np.random.rand(hidden_size, hidden_size) - 0.5) * scale).astype(np.float32)\n",
    "wt_x3 = ((np.random.rand(input_size, hidden_size) - 0.5) * scale).astype(np.float32)\n",
    "bias3 = ((np.random.rand(hidden_size) - 0.5) * scale).astype(np.float32)\n",
    "\n",
    "rnn3 = tf.keras.layers.SimpleRNN(hidden_size, \n",
    "                                 kernel_initializer=tf.keras.initializers.Constant(wt_x3),\n",
    "                                 recurrent_initializer=tf.keras.initializers.Constant(wt_h3), \n",
    "                                 bias_initializer=tf.keras.initializers.Constant(bias3),\n",
    "                                 return_sequences=True, return_state=False)\n",
    "\n",
    "\n",
    "init_state = tf.Variable(np.random.random_sample([batch_size, hidden_size]), dtype=tf.float32)\n",
    "\n",
    "with tf.GradientTape(persistent=True) as g:\n",
    "    g.watch(init_state)\n",
    "    outputs = rnn3(input_data, initial_state=init_state)\n",
    "    obj = tf.reduce_sum(tf.square(outputs[:, 30, :]))\n",
    "    \n",
    "d_init_state = g.gradient(obj, init_state) \n",
    "\n",
    "# show the norms of gradients. Most of them are zero. \n",
    "rnn_grad_norm = np.linalg.norm(d_init_state, axis=1)\n",
    "\n",
    "n_bins = 20\n",
    "_ = plt.hist(rnn_grad_norm, bins=n_bins)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GRU\n",
    "\n",
    "If a GRU is configurated in a way such that it holds its memory, then there is no gradient vanishing problem. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# set up a GRU with long-term memory\n",
    "wtz_h, wtz_x, biasz, wtr_h, wtr_x, biasr, wth_h, wth_x, biash = init_gru_with_long_term_memory(wt_h3, wt_x3, bias3)\n",
    "\n",
    "# concatenate these parameters to initialize GRU kernels\n",
    "kernel_init = np.concatenate([wtz_x, wtr_x, wth_x], axis=1)\n",
    "rec_kernel_init = np.concatenate([wtz_h, wtr_h, wth_h], axis=1)\n",
    "bias_init = np.concatenate([biasz, biasr, biash], axis=0)\n",
    "\n",
    "gru3 = tf.keras.layers.GRU(hidden_size, \n",
    "                            kernel_initializer=tf.keras.initializers.Constant(kernel_init),\n",
    "                            recurrent_initializer=tf.keras.initializers.Constant(rec_kernel_init), \n",
    "                            bias_initializer=tf.keras.initializers.Constant(bias_init),\n",
    "                            return_sequences=True, return_state=False, reset_after=False)\n",
    "\n",
    "\n",
    "\n",
    "# Feed in an initial state to the GRU and compute the gradient\n",
    "init_state = tf.Variable(np.random.random_sample([batch_size, hidden_size]), dtype=tf.float32)\n",
    "with tf.GradientTape(persistent=True) as g:\n",
    "    g.watch(init_state)\n",
    "    outputs = gru3(input_data, initial_state=init_state)\n",
    "    obj = tf.reduce_sum(tf.square(outputs[:, 30, :]))\n",
    "    \n",
    "d_init_state = g.gradient(obj, init_state) \n",
    "\n",
    "# show the norms of gradients. \n",
    "rnn_grad_norm = np.linalg.norm(d_init_state, axis=1)\n",
    "\n",
    "n_bins = 20\n",
    "_ = plt.hist(rnn_grad_norm, bins=n_bins)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Network for Modeling Sentences\n",
    "\n",
    "In this task, we will use RNNs to model sentences. The task is to predict the next character in a sentence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.set_autosave_interval(180000)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosaving every 180 seconds\n"
     ]
    }
   ],
   "source": [
    "# As usual, a bit of setup\n",
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#need to have these two lines to work on my ancient 1060 3gb\n",
    "#  https://stackoverflow.com/questions/43990046/tensorflow-blas-gemm-launch-failed\n",
    "physical_devices = tf.config.list_physical_devices('GPU') \n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%autosave 180\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total of  4328 non-ascii chars are removed \n",
      "\n",
      "Data statistics:\n",
      "Number of sentences:  160000\n",
      "Maximum and minimum sentence lengths: 100 32\n",
      "Total number of characters: 10954565\n",
      "Vocabulary size:  95\n",
      "Chars in vocabulary and their frequencies:\n",
      "[('\\n', 160000), (' ', 1762678), ('!', 12100), ('#', 496), ('$', 1212), ('%', 450), ('&', 1366), (\"'\", 88729), ('(', 8734), (')', 8890), ('*', 4310), ('+', 123), (',', 33680), ('-', 20064), ('.', 108694), ('/', 1586), ('0', 11139), ('1', 10960), ('2', 7690), ('3', 3517), ('4', 2882), ('5', 4272), ('6', 2673), ('7', 2496), ('8', 2071), ('9', 2801), (':', 22223), (';', 607), ('<', 12), ('=', 103), ('>', 9), ('?', 48816), ('@', 34), ('A', 8259), ('B', 4063), ('C', 5317), ('D', 6787), ('E', 2239), ('F', 3232), ('G', 2668), ('H', 11482), ('I', 15839), ('J', 2999), ('K', 2315), ('L', 2612), ('M', 7724), ('N', 3017), ('O', 2211), ('P', 3722), ('Q', 1036), ('R', 2942), ('S', 7281), ('T', 15062), ('U', 1014), ('V', 720), ('W', 37161), ('X', 17), ('Y', 2381), ('Z', 149), ('[', 1), ('\\\\', 25), (']', 4), ('^', 322), ('_', 107), ('`', 16), ('a', 726754), ('b', 148176), ('c', 253811), ('d', 319199), ('e', 964237), ('f', 163468), ('g', 191416), ('h', 397259), ('i', 592936), ('j', 23898), ('k', 111404), ('l', 371704), ('m', 225041), ('n', 552588), ('o', 684697), ('p', 184115), ('q', 6356), ('r', 515062), ('s', 585280), ('t', 698276), ('u', 258476), ('v', 81822), ('w', 171901), ('x', 17369), ('y', 209349), ('z', 11610), ('{', 9), ('|', 66), ('}', 12), ('~', 133)]\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import string\n",
    "import numpy as np\n",
    "\n",
    "def load_data(data_file, with_labels = False):\n",
    "    \"\"\"Load the data into a list of strings\"\"\"\n",
    "    \n",
    "    #'charmap' codec can't decode byte 0x9d in position 4051: character maps to <undefined>\n",
    "    #    fixed with adding <encoding = 'utf-8'>\n",
    "    with open(data_file, encoding = 'utf-8') as csv_file:\n",
    "        reader = csv.reader(csv_file, delimiter=',')\n",
    "        rows = list(reader)\n",
    "\n",
    "    if data_file == 'train.csv':\n",
    "        sentences, labels = zip(*rows[1:])\n",
    "        labels            = [0 if l==\"False\" else 1 for l in labels]\n",
    "        sentences = list(sentences)\n",
    "    elif data_file == 'test.csv':\n",
    "        sentences = [row[0] for row in rows[1:]]\n",
    "    else:\n",
    "        print(\"Can only load 'train.csv' or 'test.csv'\")\n",
    "    \n",
    "    # replace non ascii chars to spaces\n",
    "    count = 0\n",
    "    for i, sen in enumerate(sentences):\n",
    "        count = count + sum([0 if ord(i) < 128 else 1 for i in sen])\n",
    "        \n",
    "        # '\\n' indicates the end of the sentence\n",
    "        sentences[i] = ''.join([i if ord(i) < 128 else ' ' for i in sen]) + '\\n'\n",
    "        \n",
    "    print('The total of ', count, 'non-ascii chars are removed \\n')\n",
    "\n",
    "    if not with_labels:\n",
    "        return sentences\n",
    "    else:\n",
    "        return sentences, labels\n",
    "\n",
    "def char_to_index(sentence, str_voc):\n",
    "    \"\"\"Convert a string to an array by using the index in the vocabulary\"\"\"\n",
    "    \n",
    "    sen_int = np.array([str_voc.index(c) for c in sentence])\n",
    "    return sen_int\n",
    "\n",
    "def convert_sen_to_data(sentences, str_voc):\n",
    "    \"\"\" Convert a list of strings to a list of numpy arrays\"\"\"\n",
    "    data = [None] * len(sentences)\n",
    "    for i, sen in enumerate(sentences):\n",
    "        data[i] = char_to_index(sen, str_voc)\n",
    "        \n",
    "        # sanity check\n",
    "        #if i < 5:\n",
    "        #    recover = \"\".join([str_voc[k] for k in data[i]])\n",
    "        #    print(recover)\n",
    "    return data\n",
    "\n",
    "\n",
    "train_sentences, labels = load_data('train.csv', with_labels = True)\n",
    "# NOTE: you need to use the same vocabulary to handle your test sentences\n",
    "vocabulary = list(set(\"\".join(train_sentences))) \n",
    "vocabulary.sort()\n",
    "str_voc = \"\".join(vocabulary)\n",
    "\n",
    "train_data = convert_sen_to_data(train_sentences, str_voc)\n",
    "\n",
    "\n",
    "num_sen = len(train_data)\n",
    "sen_lengths = [sen.shape[0] for sen in train_data]\n",
    "max_len = max(sen_lengths)\n",
    "min_len = min(sen_lengths)\n",
    "num_chars = sum(sen_lengths)\n",
    "\n",
    "print('Data statistics:')\n",
    "print('Number of sentences: ', num_sen)\n",
    "print('Maximum and minimum sentence lengths:', max_len, min_len)\n",
    "print('Total number of characters:', num_chars)\n",
    "print('Vocabulary size: ', len(vocabulary))\n",
    "\n",
    "uniq, uniq_counts = np.unique(np.concatenate(train_data), return_counts=True)\n",
    "freq = np.zeros_like(uniq_counts)\n",
    "freq[uniq] = uniq_counts\n",
    "\n",
    "print('Chars in vocabulary and their frequencies:')\n",
    "print(list(zip(vocabulary, freq.tolist())))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement an RNN and a GRU with tensorflow\n",
    "\n",
    "**Q7 (10 points)** In this problem, you are supposed to train a recurrent neural network to model sentences. Particuarly, your model will receive 10 starting characters and should predict the rest of sentence. The model will be evaluated by per-character cross-entropy loss. You will get \n",
    "* 5 points if your per-character cross-entropy loss is less than 3.13 (the loss by predicting with character frequencies). \n",
    "* 8 points if your per-character cross-entropy loss is less than 2\n",
    "* 10 points if your per-character cross-entropy loss is less than 1.5\n",
    "\n",
    "\\*The performance from a [paper](https://arxiv.org/pdf/1808.04444.pdf) indicates that an LSTM can achieve performance of 1.43 * ln(2) = 0.991. \n",
    "\\*The `zip` program for compressing files roughly can achieve a performances of 3.522 bits per character. It corresponds to a performance of  3.522 * ln(2) = 2.441"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Default network given by TA\n",
    "\n",
    "from rnn_lm import masked_lm_loss\n",
    "\n",
    "voc_size = len(str_voc)\n",
    "\n",
    "\n",
    "# You don't have to do padding yourself if your model support varied lengths of sequences. \n",
    "train_mat = tf.keras.preprocessing.sequence.pad_sequences(train_data, maxlen=max_len, \n",
    "                                                         padding='post', truncating='post',\n",
    "                                                         value=-1)\n",
    "# I use a small fraction of data to train the model for a quick demo\n",
    "# You probably want to use all the data\n",
    "# train_mat = train_mat[:1600] #given\n",
    "train_mat = train_mat[:16000]\n",
    "\n",
    "\n",
    "# prepare the input and the desired output\n",
    "train_x = np.concatenate([- np.ones([train_mat.shape[0], 1]), train_mat[:, :-1]], axis=1)\n",
    "train_y = train_mat\n",
    "\n",
    "\n",
    "# construct the model\n",
    "# Here I include a Lambda layer and an embedding layer for your reference\n",
    "batch_size = 32\n",
    "model_batch = tf.keras.Sequential()\n",
    "model_batch.add(tf.keras.layers.InputLayer(batch_input_shape=(batch_size, 100, 1)))\n",
    "model_batch.add(tf.keras.layers.Lambda(lambda x: tf.squeeze(x + 1, axis=[-1])))\n",
    "model_batch.add(tf.keras.layers.Embedding(input_dim=voc_size + 1, output_dim=10, input_length=max_len))\n",
    "model_batch.add(tf.keras.layers.SimpleRNN(95, activation='tanh', return_sequences=True, stateful=False))\n",
    "\n",
    "# NOTE: the output of the model should be `[batch_size, seq_length, voc_size]`\n",
    "# `seq_length` can be either the original length if you do not pad, or the \n",
    "# length after padding\n",
    "model_batch.summary()\n",
    "model_batch.compile(optimizer=\"Adam\", loss=masked_lm_loss)\n",
    "model_batch.fit(x=train_x, y=train_y, epochs=4, batch_size=batch_size)\n",
    "\n",
    "\n",
    "# NOTE: The following code converts the trained model to a \"stateful\" one so it can do stepwise \n",
    "# predictions without forgetting previous hidden states. We do this by allocating a\n",
    "# a new model and copying weights from the trained model to this new model. \n",
    "\n",
    "# TODO: you need to do the same thing for your own model. This example only works for this example\n",
    "\n",
    "# NOTE: the batch size needs to be one because your model will be used to generate \n",
    "# a single sentence below. \n",
    "\n",
    "batch_size = 1\n",
    "model = tf.keras.Sequential()\n",
    "# NOTE: You need to use exactly the same way to construct this model as your trained model BUT set \n",
    "# `stateful=True` to EVERY recurrent layer\n",
    "\n",
    "model.add(tf.keras.layers.InputLayer(batch_input_shape=(batch_size, 100, 1)))\n",
    "model.add(tf.keras.layers.Lambda(lambda x: tf.squeeze(x + 1, axis=[-1])))\n",
    "model.add(tf.keras.layers.Embedding(input_dim=voc_size + 1, output_dim=10, input_length=max_len))\n",
    "model.add(tf.keras.layers.SimpleRNN(95, activation='tanh', return_sequences=True, stateful=True))\n",
    "\n",
    "\n",
    "# Then copy weights from the trained model to this new model\n",
    "for il, layer in enumerate(model_batch.layers):\n",
    "    model.layers[il].set_weights(layer.get_weights())\n",
    "\n",
    "\n",
    "model.save('rnn_lm.mod') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "voc_size:  95\n",
      "max_len:  100\n",
      "train_x =  [-1. 55. 65. 84. 67. 72. 26.  1. 68. 65. 82. 86. 73. 83. 72.  1. 71. 65.\n",
      " 86. 69.  1. 72. 73. 84. 84. 69. 82.  1. 87. 72. 73. 80. 76. 65. 83. 72.\n",
      "  1. 87. 73. 84. 72.  1. 83. 76. 79. 87.  1. 80. 73. 84. 67. 72.  0. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
      "Model: \"sequential_10\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lambda_10 (Lambda)           (32, 100)                 0         \n",
      "_________________________________________________________________\n",
      "embedding_10 (Embedding)     (32, 100, 1024)           98304     \n",
      "_________________________________________________________________\n",
      "gru_10 (GRU)                 (32, 100, 95)             319485    \n",
      "=================================================================\n",
      "Total params: 417,789\n",
      "Trainable params: 417,789\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "450/450 [==============================] - 10s 22ms/step - loss: 3.3829 - val_loss: 3.3370\n",
      "Epoch 2/30\n",
      "450/450 [==============================] - 10s 22ms/step - loss: 3.3160 - val_loss: 3.3040\n",
      "Epoch 3/30\n",
      "450/450 [==============================] - 10s 22ms/step - loss: 3.2992 - val_loss: 3.2996\n",
      "Epoch 4/30\n",
      "450/450 [==============================] - 10s 22ms/step - loss: 3.2969 - val_loss: 3.2994\n",
      "Epoch 5/30\n",
      "343/450 [=====================>........] - ETA: 2s - loss: 3.2947"
     ]
    }
   ],
   "source": [
    "#using custom model for sentence prediction\n",
    "\n",
    "from sentence_predictor import generate_model\n",
    "\n",
    "voc_size = len(str_voc)\n",
    "print(\"voc_size: \", voc_size)\n",
    "print(\"max_len: \", max_len)\n",
    "\n",
    "# use keras padding - NOT MANDATORY?\n",
    "train_mat = tf.keras.preprocessing.sequence.pad_sequences(train_data, maxlen=max_len, \n",
    "                                                         padding='post', truncating='post',\n",
    "                                                         value=-1)\n",
    "\n",
    "train_mat = train_mat[:16000] #debug on fraction of data\n",
    "\n",
    "# prepare the input and the desired output\n",
    "train_x = np.concatenate([- np.ones([train_mat.shape[0], 1]), train_mat[:, :-1]], axis=1)\n",
    "train_y = train_mat\n",
    "\n",
    "print(\"train_x = \", train_x[1,:])\n",
    "\n",
    "model = generate_model(train_x, train_y, voc_size, max_len)\n",
    "\n",
    "# model.save('rnn_lm.mod') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "from rnn_lm import masked_lm_loss\n",
    "\n",
    "# load the test data. NOTE: need to use the same vocabulary as the training data\n",
    "sentences = load_data('test.csv')\n",
    "\n",
    "# NOTE: To speed up the testing speed, I randomly select 1000 sentences as the test set. \n",
    "# Let me know if you get a much better performance on the entire test set. \n",
    "np.random.seed(137)\n",
    "selection = np.random.choice(len(sentences), size=1000, replace=False)\n",
    "\n",
    "# prepare the input\n",
    "test_sentences = [sentences[i] for i in selection]\n",
    "test_data = convert_sen_to_data(test_sentences, str_voc)\n",
    "test_mat = tf.keras.preprocessing.sequence.pad_sequences(test_data, maxlen=max_len, \n",
    "                                                         padding='post', truncating='post',\n",
    "                                                         value=-1)\n",
    "\n",
    "test_x = np.concatenate([- np.ones([test_mat.shape[0], 1]), test_mat[:, :-1]], axis=1)\n",
    "\n",
    "# Load your powerful model and compile it with the loss I have defined.\n",
    "# NOTE: compiling your model with my loss should not matter because I only use \n",
    "# your model for prediction. \n",
    "model = tf.keras.models.load_model('rnn_lm.mod', compile=False)\n",
    "model.compile(optimizer=\"adam\", loss=masked_lm_loss)\n",
    "\n",
    "# set batch size to 1\n",
    "batch_size = 1\n",
    "\n",
    "# Evaluate the model on test sentences in batch mode\n",
    "model.reset_states()\n",
    "batch_pred = model.predict(test_x, batch_size=1)\n",
    "losses = masked_lm_loss(test_mat, batch_pred)\n",
    "per_char_loss = np.mean(losses.numpy())\n",
    "\n",
    "# Your points will be decided by the per-char-loss\n",
    "print('predict and calculate loss:')\n",
    "print('The per-char-loss is about %f' % per_char_loss)\n",
    "\n",
    "\n",
    "# make sure that stepwise predictions are the same as batch predictions\n",
    "# test the model on a single sentence\n",
    "\n",
    "test_x_single = test_x[0:1]\n",
    "test_single = test_mat[0:1]\n",
    "\n",
    "# batch prediction\n",
    "model.reset_states()\n",
    "batch_pred = model.predict(test_x_single, batch_size = batch_size)\n",
    "\n",
    "# step-wise prediction\n",
    "model.reset_states()\n",
    "diff = 0\n",
    "for t in range(max_len):\n",
    "        \n",
    "    predict = model.predict(test_x_single[0:1, t:t+1], batch_size=1)\n",
    "       \n",
    "    max_per_entry_diff = np.max(np.abs(predict[0, 0] - batch_pred[0, t]))\n",
    "\n",
    "    if diff < max_per_entry_diff:\n",
    "        diff = max_per_entry_diff\n",
    "\n",
    "# The difference should be zero\n",
    "print('Difference between the two types of predictions is ', diff)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use the model to generate sentences\n",
    "\n",
    "Now we can use the trained model to generate text with a starting string. The naive model just predict frequent characters in the text, so there is no meaningful generation yet. See what you get from your models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, start_string, str_voc):\n",
    "    \"\"\" Generate random text from a starting string. The code is modified from this \n",
    "    [example](https://www.tensorflow.org/tutorials/text/text_generation)\"\"\"\n",
    "\n",
    "    # Number of characters to generate\n",
    "    num_generate = 100 - len(start_string)\n",
    "\n",
    "    # Converting our start string to numbers (vectorizing)\n",
    "    input_eval = np.array([str_voc.index(s) for s in start_string])\n",
    "    input_eval = np.reshape(input_eval, [1, -1, 1])\n",
    "\n",
    "    # Empty string to store our results\n",
    "    text_generated = []\n",
    "\n",
    "    # Low temperature results in more predictable text.\n",
    "    # Higher temperature results in more surprising text.\n",
    "    # Experiment to find the best setting.\n",
    "    temperature = 1.0\n",
    "\n",
    "    # Here batch size == 1\n",
    "    model.reset_states()\n",
    "    for i in range(num_generate):\n",
    "        \n",
    "        predictions = model(input_eval)\n",
    "        # remove the batch dimension\n",
    "        predictions = predictions[0]\n",
    "\n",
    "        # using a categorical distribution to predict the character returned by the model\n",
    "        predictions = predictions / temperature\n",
    "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
    "\n",
    "        # Pass the predicted character as the next input to the model\n",
    "        # along with the previous hidden state\n",
    "        input_eval = tf.reshape([predicted_id], [1, 1, 1])\n",
    "\n",
    "        text_generated.append(str_voc[predicted_id])\n",
    "\n",
    "    return (start_string + ''.join(text_generated))\n",
    "\n",
    "#default -> should be 10 char long??\n",
    "start_string = 'There '\n",
    "# start_string = 'The thing about '\n",
    "gen_sen = generate_text(model, start_string, str_voc)\n",
    "gen_sen = gen_sen.split('\\n')[0]\n",
    "\n",
    "print('Starting from \"' + start_string + '\", the generated sentence is:')\n",
    "print('\"' + gen_sen + '\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra Credit: Implement A Sentence Classifier using RNN\n",
    "\n",
    "(Q8) In this OPTIONAL problem, you need to perform sentence classification using RNN. The datasets that we use here is the same dataset that we used in the earlier text generation problem. You will get\n",
    "* 5 additional points if the test accuracy is above 70%\n",
    "* 7 additional points if the test accuracy is above 80%\n",
    "* 9 additional points if the test accuracy is above 85%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating Test and Train Datasets\n",
    "all_sentences, all_labels = load_data('train.csv', with_labels = True)\n",
    "# NOTE: you need to use the same vocabulary to handle your test sentences\n",
    "vocabulary = list(set(\"\".join(all_sentences))) \n",
    "vocabulary.sort()\n",
    "str_voc = \"\".join(vocabulary)\n",
    "train_data = convert_sen_to_data(train_sentences, str_voc)\n",
    "train_mat = tf.keras.preprocessing.sequence.pad_sequences(train_data, maxlen=100, \n",
    "                                                         padding='post', truncating='post',\n",
    "                                                         value=-1)\n",
    "\n",
    "print(f\"The shape of the complete dataset: {train_mat.shape}\")\n",
    "# The training and test examples\n",
    "x_train  = train_mat[:80000]\n",
    "x_test   = train_mat[80000:160000]\n",
    "\n",
    "# The training and test labels\n",
    "y_train  = np.array(all_labels[:80000])\n",
    "y_test   = np.array(all_labels[80000:160000])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example Model\n",
    "# You can update your model here\n",
    "model_c = tf.keras.Sequential([\n",
    "    tf.keras.layers.InputLayer(input_shape=(100,)),\n",
    "    tf.keras.layers.Lambda(lambda x: x+1),\n",
    "    tf.keras.layers.Embedding(\n",
    "        input_dim=voc_size + 1,\n",
    "        output_dim=10,\n",
    "        # Use masking to handle the variable sequence lengths\n",
    "        mask_zero=True),\n",
    "    tf.keras.layers.SimpleRNN(10),\n",
    "    tf.keras.layers.Dense(10, activation='relu'),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "model_c.summary()\n",
    "\n",
    "# Compile and Fit the model\n",
    "model_c.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=False),\n",
    "              optimizer=tf.keras.optimizers.Adam(1e-4),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Training on only 100 datasets: You need to run the model on all the training data for \n",
    "# higher classification accuracy\n",
    "model_c.fit(x=x_train[:100], y = y_train[:100], epochs = 200, batch_size = 32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_c.save(\"rnn_classifier.kmod\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing test accuracy\n",
    "model_cl = tf.keras.models.load_model(\"rnn_classifier.kmod\")\n",
    "y_pred = (model_cl(x_test) > 0.5).numpy().flatten().astype(int)\n",
    "accuracy = np.sum(y_test == y_pred) / y_pred.shape[0]\n",
    "print(f\"Accuracy : {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

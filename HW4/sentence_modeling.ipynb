{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Network for Modeling Sentences\n",
    "\n",
    "In this task, we will use RNNs to model sentences. The task is to predict the next character in a sentence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.set_autosave_interval(180000)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosaving every 180 seconds\n"
     ]
    }
   ],
   "source": [
    "# As usual, a bit of setup\n",
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#need to have these two lines to work on my ancient 1060 3gb\n",
    "#  https://stackoverflow.com/questions/43990046/tensorflow-blas-gemm-launch-failed\n",
    "physical_devices = tf.config.list_physical_devices('GPU') \n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%autosave 180\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total of  4328 non-ascii chars are removed \n",
      "\n",
      "Data statistics:\n",
      "Number of sentences:  160000\n",
      "Maximum and minimum sentence lengths: 100 32\n",
      "Total number of characters: 10954565\n",
      "Vocabulary size:  95\n",
      "Chars in vocabulary and their frequencies:\n",
      "[('\\n', 160000), (' ', 1762678), ('!', 12100), ('#', 496), ('$', 1212), ('%', 450), ('&', 1366), (\"'\", 88729), ('(', 8734), (')', 8890), ('*', 4310), ('+', 123), (',', 33680), ('-', 20064), ('.', 108694), ('/', 1586), ('0', 11139), ('1', 10960), ('2', 7690), ('3', 3517), ('4', 2882), ('5', 4272), ('6', 2673), ('7', 2496), ('8', 2071), ('9', 2801), (':', 22223), (';', 607), ('<', 12), ('=', 103), ('>', 9), ('?', 48816), ('@', 34), ('A', 8259), ('B', 4063), ('C', 5317), ('D', 6787), ('E', 2239), ('F', 3232), ('G', 2668), ('H', 11482), ('I', 15839), ('J', 2999), ('K', 2315), ('L', 2612), ('M', 7724), ('N', 3017), ('O', 2211), ('P', 3722), ('Q', 1036), ('R', 2942), ('S', 7281), ('T', 15062), ('U', 1014), ('V', 720), ('W', 37161), ('X', 17), ('Y', 2381), ('Z', 149), ('[', 1), ('\\\\', 25), (']', 4), ('^', 322), ('_', 107), ('`', 16), ('a', 726754), ('b', 148176), ('c', 253811), ('d', 319199), ('e', 964237), ('f', 163468), ('g', 191416), ('h', 397259), ('i', 592936), ('j', 23898), ('k', 111404), ('l', 371704), ('m', 225041), ('n', 552588), ('o', 684697), ('p', 184115), ('q', 6356), ('r', 515062), ('s', 585280), ('t', 698276), ('u', 258476), ('v', 81822), ('w', 171901), ('x', 17369), ('y', 209349), ('z', 11610), ('{', 9), ('|', 66), ('}', 12), ('~', 133)]\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import string\n",
    "import numpy as np\n",
    "\n",
    "def load_data(data_file, with_labels = False):\n",
    "    \"\"\"Load the data into a list of strings\"\"\"\n",
    "    \n",
    "    #'charmap' codec can't decode byte 0x9d in position 4051: character maps to <undefined>\n",
    "    #    fixed with adding <encoding = 'utf-8'>\n",
    "    with open(data_file, encoding = 'utf-8') as csv_file:\n",
    "        reader = csv.reader(csv_file, delimiter=',')\n",
    "        rows = list(reader)\n",
    "\n",
    "    if data_file == 'train.csv':\n",
    "        sentences, labels = zip(*rows[1:])\n",
    "        labels            = [0 if l==\"False\" else 1 for l in labels]\n",
    "        sentences = list(sentences)\n",
    "    elif data_file == 'test.csv':\n",
    "        sentences = [row[0] for row in rows[1:]]\n",
    "    else:\n",
    "        print(\"Can only load 'train.csv' or 'test.csv'\")\n",
    "    \n",
    "    # replace non ascii chars to spaces\n",
    "    count = 0\n",
    "    for i, sen in enumerate(sentences):\n",
    "        count = count + sum([0 if ord(i) < 128 else 1 for i in sen])\n",
    "        \n",
    "        # '\\n' indicates the end of the sentence\n",
    "        sentences[i] = ''.join([i if ord(i) < 128 else ' ' for i in sen]) + '\\n'\n",
    "        \n",
    "    print('The total of ', count, 'non-ascii chars are removed \\n')\n",
    "\n",
    "    if not with_labels:\n",
    "        return sentences\n",
    "    else:\n",
    "        return sentences, labels\n",
    "\n",
    "def char_to_index(sentence, str_voc):\n",
    "    \"\"\"Convert a string to an array by using the index in the vocabulary\"\"\"\n",
    "    \n",
    "    sen_int = np.array([str_voc.index(c) for c in sentence])\n",
    "    return sen_int\n",
    "\n",
    "def convert_sen_to_data(sentences, str_voc):\n",
    "    \"\"\" Convert a list of strings to a list of numpy arrays\"\"\"\n",
    "    data = [None] * len(sentences)\n",
    "    for i, sen in enumerate(sentences):\n",
    "        data[i] = char_to_index(sen, str_voc)\n",
    "        \n",
    "        # sanity check\n",
    "        #if i < 5:\n",
    "        #    recover = \"\".join([str_voc[k] for k in data[i]])\n",
    "        #    print(recover)\n",
    "    return data\n",
    "\n",
    "\n",
    "train_sentences, labels = load_data('train.csv', with_labels = True)\n",
    "# NOTE: you need to use the same vocabulary to handle your test sentences\n",
    "vocabulary = list(set(\"\".join(train_sentences))) \n",
    "vocabulary.sort()\n",
    "str_voc = \"\".join(vocabulary)\n",
    "\n",
    "train_data = convert_sen_to_data(train_sentences, str_voc)\n",
    "\n",
    "\n",
    "num_sen = len(train_data)\n",
    "sen_lengths = [sen.shape[0] for sen in train_data]\n",
    "max_len = max(sen_lengths)\n",
    "min_len = min(sen_lengths)\n",
    "num_chars = sum(sen_lengths)\n",
    "\n",
    "print('Data statistics:')\n",
    "print('Number of sentences: ', num_sen)\n",
    "print('Maximum and minimum sentence lengths:', max_len, min_len)\n",
    "print('Total number of characters:', num_chars)\n",
    "print('Vocabulary size: ', len(vocabulary))\n",
    "\n",
    "uniq, uniq_counts = np.unique(np.concatenate(train_data), return_counts=True)\n",
    "freq = np.zeros_like(uniq_counts)\n",
    "freq[uniq] = uniq_counts\n",
    "\n",
    "print('Chars in vocabulary and their frequencies:')\n",
    "print(list(zip(vocabulary, freq.tolist())))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement an RNN and a GRU with tensorflow\n",
    "\n",
    "**Q7 (10 points)** In this problem, you are supposed to train a recurrent neural network to model sentences. Particuarly, your model will receive 10 starting characters and should predict the rest of sentence. The model will be evaluated by per-character cross-entropy loss. You will get \n",
    "* 5 points if your per-character cross-entropy loss is less than 3.13 (the loss by predicting with character frequencies). \n",
    "* 8 points if your per-character cross-entropy loss is less than 2\n",
    "* 10 points if your per-character cross-entropy loss is less than 1.5\n",
    "\n",
    "\\*The performance from a [paper](https://arxiv.org/pdf/1808.04444.pdf) indicates that an LSTM can achieve performance of 1.43 * ln(2) = 0.991. \n",
    "\\*The `zip` program for compressing files roughly can achieve a performances of 3.522 bits per character. It corresponds to a performance of  3.522 * ln(2) = 2.441"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Default network given by TA\n",
    "\n",
    "from rnn_lm import masked_lm_loss\n",
    "\n",
    "voc_size = len(str_voc)\n",
    "\n",
    "\n",
    "# You don't have to do padding yourself if your model support varied lengths of sequences. \n",
    "train_mat = tf.keras.preprocessing.sequence.pad_sequences(train_data, maxlen=max_len, \n",
    "                                                         padding='post', truncating='post',\n",
    "                                                         value=-1)\n",
    "# I use a small fraction of data to train the model for a quick demo\n",
    "# You probably want to use all the data\n",
    "# train_mat = train_mat[:1600] #given\n",
    "train_mat = train_mat[:16000]\n",
    "\n",
    "\n",
    "# prepare the input and the desired output\n",
    "train_x = np.concatenate([- np.ones([train_mat.shape[0], 1]), train_mat[:, :-1]], axis=1)\n",
    "\n",
    "train_y = train_mat\n",
    "\n",
    "\n",
    "# construct the model\n",
    "# Here I include a Lambda layer and an embedding layer for your reference\n",
    "batch_size = 32\n",
    "model_batch = tf.keras.Sequential()\n",
    "model_batch.add(tf.keras.layers.InputLayer(batch_input_shape=(batch_size, 100, 1)))\n",
    "model_batch.add(tf.keras.layers.Lambda(lambda x: tf.squeeze(x + 1, axis=[-1])))\n",
    "model_batch.add(tf.keras.layers.Embedding(input_dim=voc_size + 1, output_dim=10, input_length=max_len))\n",
    "model_batch.add(tf.keras.layers.SimpleRNN(95, activation='tanh', return_sequences=True, stateful=False))\n",
    "\n",
    "# NOTE: the output of the model should be `[batch_size, seq_length, voc_size]`\n",
    "# `seq_length` can be either the original length if you do not pad, or the \n",
    "# length after padding\n",
    "model_batch.summary()\n",
    "model_batch.compile(optimizer=\"Adam\", loss=masked_lm_loss)\n",
    "model_batch.fit(x=train_x, y=train_y, epochs=4, batch_size=batch_size)\n",
    "\n",
    "\n",
    "# NOTE: The following code converts the trained model to a \"stateful\" one so it can do stepwise \n",
    "# predictions without forgetting previous hidden states. We do this by allocating a\n",
    "# a new model and copying weights from the trained model to this new model. \n",
    "\n",
    "# TODO: you need to do the same thing for your own model. This example only works for this example\n",
    "\n",
    "# NOTE: the batch size needs to be one because your model will be used to generate \n",
    "# a single sentence below. \n",
    "\n",
    "batch_size = 1\n",
    "model = tf.keras.Sequential()\n",
    "# NOTE: You need to use exactly the same way to construct this model as your trained model BUT set \n",
    "# `stateful=True` to EVERY recurrent layer\n",
    "\n",
    "model.add(tf.keras.layers.InputLayer(batch_input_shape=(batch_size, 100, 1)))\n",
    "model.add(tf.keras.layers.Lambda(lambda x: tf.squeeze(x + 1, axis=[-1])))\n",
    "model.add(tf.keras.layers.Embedding(input_dim=voc_size + 1, output_dim=10, input_length=max_len))\n",
    "model.add(tf.keras.layers.SimpleRNN(95, activation='tanh', return_sequences=True, stateful=True))\n",
    "\n",
    "\n",
    "# Then copy weights from the trained model to this new model\n",
    "for il, layer in enumerate(model_batch.layers):\n",
    "    model.layers[il].set_weights(layer.get_weights())\n",
    "\n",
    "\n",
    "model.save('rnn_lm.mod') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "voc_size:  95\n",
      "max_len:  100\n",
      "train_x =  [ 0. 55. 65. 84. 67. 72. 26.  1. 68. 65. 82. 86. 73. 83. 72.  1. 71. 65.\n",
      " 86. 69.  1. 72. 73. 84. 84. 69. 82.  1. 87. 72. 73. 80. 76. 65. 83. 72.\n",
      "  1. 87. 73. 84. 72.  1. 83. 76. 79. 87.  1. 80. 73. 84. 67. 72.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "train_y =  [55 65 84 67 72 26  1 68 65 82 86 73 83 72  1 71 65 86 69  1 72 73 84 84\n",
      " 69 82  1 87 72 73 80 76 65 83 72  1 87 73 84 72  1 83 76 79 87  1 80 73\n",
      " 84 67 72  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0]\n",
      "Model: \"sequential_14\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lambda_14 (Lambda)           (64, 100)                 0         \n",
      "_________________________________________________________________\n",
      "embedding_14 (Embedding)     (64, 100, 128)            12288     \n",
      "_________________________________________________________________\n",
      "gru_14 (GRU)                 (64, 100, 256)            296448    \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (64, 100, 256)            0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (64, 100, 95)             24415     \n",
      "=================================================================\n",
      "Total params: 333,151\n",
      "Trainable params: 333,151\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "2250/2250 [==============================] - 40s 18ms/step - loss: 1.5137 - val_loss: 1.1971\n",
      "Epoch 2/100\n",
      "2250/2250 [==============================] - 40s 18ms/step - loss: 1.1489 - val_loss: 1.0633\n",
      "Epoch 3/100\n",
      "2250/2250 [==============================] - 40s 18ms/step - loss: 1.0724 - val_loss: 1.0206\n",
      "Epoch 4/100\n",
      "2250/2250 [==============================] - 40s 18ms/step - loss: 1.0406 - val_loss: 0.9995\n",
      "Epoch 5/100\n",
      "2250/2250 [==============================] - 40s 18ms/step - loss: 1.0235 - val_loss: 0.9868\n",
      "Epoch 6/100\n",
      "2250/2250 [==============================] - 40s 18ms/step - loss: 1.0126 - val_loss: 0.9784\n",
      "Epoch 7/100\n",
      "2250/2250 [==============================] - 40s 18ms/step - loss: 1.0050 - val_loss: 0.9721\n",
      "Epoch 8/100\n",
      "2250/2250 [==============================] - 40s 18ms/step - loss: 0.9994 - val_loss: 0.9677\n",
      "Epoch 9/100\n",
      "2250/2250 [==============================] - 40s 18ms/step - loss: 0.9947 - val_loss: 0.9651\n",
      "Epoch 10/100\n",
      "2250/2250 [==============================] - 40s 18ms/step - loss: 0.9913 - val_loss: 0.9621\n",
      "Epoch 11/100\n",
      "2250/2250 [==============================] - 40s 18ms/step - loss: 0.9883 - val_loss: 0.9594\n",
      "Epoch 12/100\n",
      "2250/2250 [==============================] - 40s 18ms/step - loss: 0.9858 - val_loss: 0.9578\n",
      "Epoch 13/100\n",
      "2250/2250 [==============================] - 40s 18ms/step - loss: 0.9838 - val_loss: 0.9556\n",
      "Epoch 14/100\n",
      "2250/2250 [==============================] - 40s 18ms/step - loss: 0.9818 - val_loss: 0.9543\n",
      "Epoch 15/100\n",
      "2250/2250 [==============================] - 40s 18ms/step - loss: 0.9800 - val_loss: 0.9529\n",
      "Epoch 16/100\n",
      "2250/2250 [==============================] - 40s 18ms/step - loss: 0.9787 - val_loss: 0.9515\n",
      "Epoch 17/100\n",
      "2250/2250 [==============================] - 40s 18ms/step - loss: 0.9774 - val_loss: 0.9506\n",
      "Epoch 18/100\n",
      "2250/2250 [==============================] - 40s 18ms/step - loss: 0.9761 - val_loss: 0.9494\n",
      "Epoch 19/100\n",
      "2250/2250 [==============================] - 40s 18ms/step - loss: 0.9751 - val_loss: 0.9493\n",
      "Epoch 20/100\n",
      "2250/2250 [==============================] - 40s 18ms/step - loss: 0.9741 - val_loss: 0.9486\n",
      "Epoch 21/100\n",
      "2250/2250 [==============================] - 40s 18ms/step - loss: 0.9733 - val_loss: 0.9472\n",
      "Epoch 22/100\n",
      "2250/2250 [==============================] - 40s 18ms/step - loss: 0.9724 - val_loss: 0.9465\n",
      "Epoch 23/100\n",
      "2250/2250 [==============================] - 40s 18ms/step - loss: 0.9716 - val_loss: 0.9457\n",
      "Epoch 24/100\n",
      "2250/2250 [==============================] - 40s 18ms/step - loss: 0.9707 - val_loss: 0.9453\n",
      "Epoch 25/100\n",
      "2250/2250 [==============================] - 40s 18ms/step - loss: 0.9701 - val_loss: 0.9448\n",
      "Epoch 26/100\n",
      "2250/2250 [==============================] - 40s 18ms/step - loss: 0.9695 - val_loss: 0.9444\n",
      "Epoch 27/100\n",
      "2250/2250 [==============================] - 40s 18ms/step - loss: 0.9688 - val_loss: 0.9432\n",
      "Epoch 28/100\n",
      "2250/2250 [==============================] - 40s 18ms/step - loss: 0.9682 - val_loss: 0.9432\n",
      "Epoch 29/100\n",
      "2250/2250 [==============================] - 40s 18ms/step - loss: 0.9678 - val_loss: 0.9426\n",
      "Epoch 30/100\n",
      "2250/2250 [==============================] - 40s 18ms/step - loss: 0.9673 - val_loss: 0.9421\n",
      "Epoch 31/100\n",
      "2250/2250 [==============================] - 40s 18ms/step - loss: 0.9667 - val_loss: 0.9418\n",
      "Epoch 32/100\n",
      "2250/2250 [==============================] - 40s 18ms/step - loss: 0.9663 - val_loss: 0.9415\n",
      "Epoch 33/100\n",
      "2250/2250 [==============================] - 40s 18ms/step - loss: 0.9657 - val_loss: 0.9416\n",
      "Epoch 34/100\n",
      "2250/2250 [==============================] - 40s 18ms/step - loss: 0.9520 - val_loss: 0.9294\n",
      "Epoch 35/100\n",
      "2250/2250 [==============================] - 40s 18ms/step - loss: 0.9492 - val_loss: 0.9281\n",
      "Epoch 36/100\n",
      "2250/2250 [==============================] - 40s 18ms/step - loss: 0.9482 - val_loss: 0.9275\n",
      "Epoch 37/100\n",
      "2250/2250 [==============================] - 40s 18ms/step - loss: 0.9477 - val_loss: 0.9271\n",
      "Epoch 38/100\n",
      "2250/2250 [==============================] - 40s 18ms/step - loss: 0.9473 - val_loss: 0.9267\n",
      "Epoch 39/100\n",
      "2250/2250 [==============================] - 40s 18ms/step - loss: 0.9470 - val_loss: 0.9263\n",
      "Epoch 40/100\n",
      "2250/2250 [==============================] - 40s 18ms/step - loss: 0.9468 - val_loss: 0.9262\n",
      "Epoch 41/100\n",
      "2250/2250 [==============================] - 40s 18ms/step - loss: 0.9465 - val_loss: 0.9260\n",
      "Epoch 42/100\n",
      "2250/2250 [==============================] - 40s 18ms/step - loss: 0.9462 - val_loss: 0.9257\n",
      "Epoch 43/100\n",
      "2250/2250 [==============================] - 40s 18ms/step - loss: 0.9461 - val_loss: 0.9255\n",
      "Epoch 44/100\n",
      "2250/2250 [==============================] - 40s 18ms/step - loss: 0.9460 - val_loss: 0.9254\n",
      "Epoch 45/100\n",
      "2250/2250 [==============================] - 40s 18ms/step - loss: 0.9458 - val_loss: 0.9252\n",
      "Epoch 46/100\n",
      "2250/2250 [==============================] - 40s 18ms/step - loss: 0.9458 - val_loss: 0.9250\n",
      "Epoch 47/100\n",
      "2250/2250 [==============================] - 40s 18ms/step - loss: 0.9456 - val_loss: 0.9250\n",
      "Epoch 48/100\n",
      "2250/2250 [==============================] - 40s 18ms/step - loss: 0.9456 - val_loss: 0.9249\n",
      "Epoch 49/100\n",
      "2250/2250 [==============================] - 40s 18ms/step - loss: 0.9454 - val_loss: 0.9248\n",
      "Epoch 50/100\n",
      "2250/2250 [==============================] - 40s 18ms/step - loss: 0.9452 - val_loss: 0.9248\n",
      "Epoch 51/100\n",
      "2250/2250 [==============================] - 40s 18ms/step - loss: 0.9451 - val_loss: 0.9245\n",
      "Epoch 52/100\n",
      "2250/2250 [==============================] - 40s 18ms/step - loss: 0.9450 - val_loss: 0.9246\n",
      "Epoch 53/100\n",
      "2250/2250 [==============================] - 40s 18ms/step - loss: 0.9450 - val_loss: 0.9243\n",
      "Epoch 54/100\n",
      "2250/2250 [==============================] - 40s 18ms/step - loss: 0.9449 - val_loss: 0.9244\n",
      "Epoch 55/100\n",
      "2250/2250 [==============================] - 40s 18ms/step - loss: 0.9448 - val_loss: 0.9245\n",
      "Epoch 56/100\n",
      "2250/2250 [==============================] - 40s 18ms/step - loss: 0.9447 - val_loss: 0.9242\n",
      "Epoch 57/100\n",
      "2250/2250 [==============================] - 40s 18ms/step - loss: 0.9447 - val_loss: 0.9242\n",
      "Epoch 58/100\n",
      "2250/2250 [==============================] - 40s 18ms/step - loss: 0.9445 - val_loss: 0.9241\n",
      "Epoch 59/100\n",
      "2250/2250 [==============================] - 40s 18ms/step - loss: 0.9446 - val_loss: 0.9241\n",
      "Epoch 60/100\n",
      "2250/2250 [==============================] - 40s 18ms/step - loss: 0.9444 - val_loss: 0.9240\n",
      "Epoch 61/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2250/2250 [==============================] - 39s 18ms/step - loss: 0.9444 - val_loss: 0.9239\n",
      "Epoch 62/100\n",
      "2250/2250 [==============================] - 39s 18ms/step - loss: 0.9443 - val_loss: 0.9238\n",
      "Epoch 63/100\n",
      "2250/2250 [==============================] - 39s 17ms/step - loss: 0.9441 - val_loss: 0.9239\n",
      "Epoch 64/100\n",
      "2250/2250 [==============================] - 39s 17ms/step - loss: 0.9441 - val_loss: 0.9237\n",
      "Epoch 65/100\n",
      "2250/2250 [==============================] - 39s 17ms/step - loss: 0.9441 - val_loss: 0.9237\n",
      "Epoch 66/100\n",
      "2250/2250 [==============================] - 39s 17ms/step - loss: 0.9440 - val_loss: 0.9236\n",
      "Epoch 67/100\n",
      "2250/2250 [==============================] - 39s 17ms/step - loss: 0.9424 - val_loss: 0.9229\n",
      "Epoch 68/100\n",
      "2250/2250 [==============================] - 39s 17ms/step - loss: 0.9422 - val_loss: 0.9229\n",
      "Epoch 69/100\n",
      "2250/2250 [==============================] - 39s 17ms/step - loss: 0.9421 - val_loss: 0.9228\n",
      "Epoch 70/100\n",
      "2250/2250 [==============================] - 39s 18ms/step - loss: 0.9421 - val_loss: 0.9227\n",
      "Epoch 71/100\n",
      "2250/2250 [==============================] - 39s 18ms/step - loss: 0.9420 - val_loss: 0.9227\n",
      "Epoch 72/100\n",
      "2250/2250 [==============================] - 39s 18ms/step - loss: 0.9420 - val_loss: 0.9227\n",
      "Epoch 73/100\n",
      "2250/2250 [==============================] - 39s 18ms/step - loss: 0.9420 - val_loss: 0.9227\n",
      "Epoch 74/100\n",
      "2250/2250 [==============================] - 39s 17ms/step - loss: 0.9421 - val_loss: 0.9227\n",
      "Epoch 75/100\n",
      "2250/2250 [==============================] - 39s 18ms/step - loss: 0.9419 - val_loss: 0.9226\n",
      "Epoch 76/100\n",
      "2250/2250 [==============================] - 39s 18ms/step - loss: 0.9419 - val_loss: 0.9226\n",
      "Epoch 77/100\n",
      "2250/2250 [==============================] - 39s 17ms/step - loss: 0.9419 - val_loss: 0.9226\n",
      "Epoch 78/100\n",
      "2250/2250 [==============================] - 39s 17ms/step - loss: 0.9418 - val_loss: 0.9226\n",
      "Epoch 79/100\n",
      "2250/2250 [==============================] - 39s 17ms/step - loss: 0.9418 - val_loss: 0.9226\n",
      "Epoch 80/100\n",
      "2250/2250 [==============================] - 40s 18ms/step - loss: 0.9418 - val_loss: 0.9225\n",
      "Epoch 81/100\n",
      "2250/2250 [==============================] - 39s 17ms/step - loss: 0.9419 - val_loss: 0.9225\n",
      "Epoch 82/100\n",
      "2250/2250 [==============================] - 39s 17ms/step - loss: 0.9417 - val_loss: 0.9225\n",
      "Epoch 83/100\n",
      "2250/2250 [==============================] - 39s 18ms/step - loss: 0.9419 - val_loss: 0.9225\n",
      "Epoch 84/100\n",
      "2250/2250 [==============================] - 39s 18ms/step - loss: 0.9417 - val_loss: 0.9225\n",
      "Epoch 85/100\n",
      "2250/2250 [==============================] - 39s 18ms/step - loss: 0.9418 - val_loss: 0.9225\n",
      "Epoch 86/100\n",
      "2250/2250 [==============================] - 39s 17ms/step - loss: 0.9416 - val_loss: 0.9225\n",
      "Epoch 87/100\n",
      "2250/2250 [==============================] - 39s 18ms/step - loss: 0.9416 - val_loss: 0.9224\n",
      "Epoch 88/100\n",
      "2250/2250 [==============================] - 39s 18ms/step - loss: 0.9418 - val_loss: 0.9224\n",
      "Epoch 89/100\n",
      "2250/2250 [==============================] - 39s 18ms/step - loss: 0.9418 - val_loss: 0.9224\n",
      "Epoch 90/100\n",
      "2250/2250 [==============================] - 39s 17ms/step - loss: 0.9417 - val_loss: 0.9224\n",
      "Epoch 91/100\n",
      "2250/2250 [==============================] - 39s 17ms/step - loss: 0.9417 - val_loss: 0.9224\n",
      "Epoch 92/100\n",
      "2250/2250 [==============================] - 39s 17ms/step - loss: 0.9418 - val_loss: 0.9224\n",
      "Epoch 93/100\n",
      "2250/2250 [==============================] - 39s 18ms/step - loss: 0.9417 - val_loss: 0.9224\n",
      "Epoch 94/100\n",
      "2250/2250 [==============================] - 39s 17ms/step - loss: 0.9417 - val_loss: 0.9224\n",
      "Epoch 95/100\n",
      "2250/2250 [==============================] - 39s 17ms/step - loss: 0.9417 - val_loss: 0.9224\n",
      "Epoch 96/100\n",
      "2250/2250 [==============================] - 39s 17ms/step - loss: 0.9416 - val_loss: 0.9224\n",
      "Epoch 97/100\n",
      "2250/2250 [==============================] - 39s 18ms/step - loss: 0.9417 - val_loss: 0.9223\n",
      "Epoch 98/100\n",
      "2250/2250 [==============================] - 39s 17ms/step - loss: 0.9417 - val_loss: 0.9223\n",
      "Epoch 99/100\n",
      "2250/2250 [==============================] - 39s 17ms/step - loss: 0.9415 - val_loss: 0.9223\n",
      "Epoch 100/100\n",
      "2250/2250 [==============================] - 39s 17ms/step - loss: 0.9417 - val_loss: 0.9223\n"
     ]
    }
   ],
   "source": [
    "#using custom model for sentence prediction\n",
    "from sentence_predictor import generate_model\n",
    "# from rnn_lm import MyModel\n",
    "\n",
    "voc_size = len(str_voc)\n",
    "print(\"voc_size: \", voc_size)\n",
    "print(\"max_len: \", max_len)\n",
    "\n",
    "# use keras padding\n",
    "train_mat = tf.keras.preprocessing.sequence.pad_sequences(train_data, maxlen=max_len, \n",
    "                                                         padding='post', truncating='post',\n",
    "                                                         value=0) #value was -1\n",
    "\n",
    "# train_mat = train_mat[:16000] #debug on fraction of data\n",
    "\n",
    "# prepare the input and the desired output\n",
    "train_x = np.concatenate([ np.zeros([train_mat.shape[0], 1]), train_mat[:, :-1]], axis=1)\n",
    "train_y = train_mat\n",
    "\n",
    "print(\"train_x = \", train_x[1,:])\n",
    "print(\"train_y = \", train_y[1,:])\n",
    "\n",
    "#old version\n",
    "model = generate_model(train_x, train_y, voc_size, max_len)\n",
    "\n",
    "#new attempt\n",
    "# BS = 16 #batch size\n",
    "\n",
    "# model_batch = MyModel(vocSize = voc_size, batchSize = BS, embedding_dim = 256, rnn_units = 1024)\n",
    "# model_batch.summary()\n",
    "\n",
    "# model_batch.compile(optimizer=tf.keras.optimizers.Adam(lr = 0.01), loss=masked_lm_loss)\n",
    "\n",
    "# model_batch.fit(train_x,train_y,epochs = 10, shuffle = True, validation_split=0.1)\n",
    "\n",
    "# model = MyModel(vocSize = voc_size, embedding_dim = 256, rnn_units = 1024)\n",
    "# #copy weights from the trained model to this new model\n",
    "# for il, layer in enumerate(model_batch.layers):\n",
    "#     model.layers[il].set_weights(layer.get_weights())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: GRU_.mod\\assets\n"
     ]
    }
   ],
   "source": [
    "model.save('GRU_.mod') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total of  1131 non-ascii chars are removed \n",
      "\n",
      "predict and calculate loss:\n",
      "The per-char-loss is about 1.858371\n",
      "WARNING:tensorflow:Model was constructed with shape (1, 100, 1) for input Tensor(\"input_16_2:0\", shape=(1, 100, 1), dtype=float32), but it was called on an input with incompatible shape (1, 1, 1).\n",
      "Difference between the two types of predictions is  7.6293945e-06\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from rnn_lm import masked_lm_loss\n",
    "\n",
    "# load the test data. NOTE: need to use the same vocabulary as the training data\n",
    "sentences = load_data('test.csv')\n",
    "\n",
    "# NOTE: To speed up the testing speed, I randomly select 1000 sentences as the test set. \n",
    "# Let me know if you get a much better performance on the entire test set. \n",
    "np.random.seed(137)\n",
    "selection = np.random.choice(len(sentences), size=1000, replace=False)\n",
    "\n",
    "# prepare the input\n",
    "test_sentences = [sentences[i] for i in selection]\n",
    "test_data = convert_sen_to_data(test_sentences, str_voc)\n",
    "test_mat = tf.keras.preprocessing.sequence.pad_sequences(test_data, maxlen=max_len, \n",
    "                                                         padding='post', truncating='post',\n",
    "                                                         value=-1)\n",
    "\n",
    "test_x = np.concatenate([- np.ones([test_mat.shape[0], 1]), test_mat[:, :-1]], axis=1)\n",
    "\n",
    "# Load your powerful model and compile it with the loss I have defined.\n",
    "# NOTE: compiling your model with my loss should not matter because I only use \n",
    "# your model for prediction. \n",
    "model = tf.keras.models.load_model('GRU_.mod', compile=False)\n",
    "model.compile(optimizer=\"adam\", loss=masked_lm_loss)\n",
    "\n",
    "# set batch size to 1\n",
    "batch_size = 1\n",
    "\n",
    "# Evaluate the model on test sentences in batch mode\n",
    "model.reset_states()\n",
    "batch_pred = model.predict(test_x, batch_size=1)\n",
    "losses = masked_lm_loss(test_mat, batch_pred)\n",
    "per_char_loss = np.mean(losses.numpy())\n",
    "\n",
    "# Your points will be decided by the per-char-loss\n",
    "print('predict and calculate loss:')\n",
    "print('The per-char-loss is about %f' % per_char_loss)\n",
    "\n",
    "\n",
    "# make sure that stepwise predictions are the same as batch predictions\n",
    "# test the model on a single sentence\n",
    "\n",
    "test_x_single = test_x[0:1]\n",
    "test_single = test_mat[0:1]\n",
    "\n",
    "# batch prediction\n",
    "model.reset_states()\n",
    "batch_pred = model.predict(test_x_single, batch_size = batch_size)\n",
    "\n",
    "# step-wise prediction\n",
    "model.reset_states()\n",
    "diff = 0\n",
    "for t in range(max_len):\n",
    "        \n",
    "    predict = model.predict(test_x_single[0:1, t:t+1], batch_size=1)\n",
    "       \n",
    "    max_per_entry_diff = np.max(np.abs(predict[0, 0] - batch_pred[0, t]))\n",
    "\n",
    "    if diff < max_per_entry_diff:\n",
    "        diff = max_per_entry_diff\n",
    "\n",
    "# The difference should be zero\n",
    "print('Difference between the two types of predictions is ', diff)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use the model to generate sentences\n",
    "\n",
    "Now we can use the trained model to generate text with a starting string. The naive model just predict frequent characters in the text, so there is no meaningful generation yet. See what you get from your models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting from \"The little \", the generated sentence is:\n",
      "\"The little with your mothers!\"\n"
     ]
    }
   ],
   "source": [
    "def generate_text(model, start_string, str_voc):\n",
    "    \"\"\" Generate random text from a starting string. The code is modified from this \n",
    "    [example](https://www.tensorflow.org/tutorials/text/text_generation)\"\"\"\n",
    "\n",
    "    # Number of characters to generate\n",
    "    num_generate = 100 - len(start_string)\n",
    "\n",
    "    # Converting our start string to numbers (vectorizing)\n",
    "    input_eval = np.array([str_voc.index(s) for s in start_string])\n",
    "    input_eval = np.reshape(input_eval, [1, -1, 1])\n",
    "\n",
    "    # Empty string to store our results\n",
    "    text_generated = []\n",
    "\n",
    "    # Low temperature results in more predictable text.\n",
    "    # Higher temperature results in more surprising text.\n",
    "    # Experiment to find the best setting.\n",
    "#     temperature = 1.0\n",
    "    temperature = 0.75\n",
    "\n",
    "\n",
    "    # Here batch size == 1\n",
    "    model.reset_states()\n",
    "    for i in range(num_generate):\n",
    "        \n",
    "        predictions = model(input_eval)\n",
    "        # remove the batch dimension\n",
    "        predictions = predictions[0]\n",
    "\n",
    "        # using a categorical distribution to predict the character returned by the model\n",
    "        predictions = predictions / temperature\n",
    "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
    "\n",
    "        # Pass the predicted character as the next input to the model\n",
    "        # along with the previous hidden state\n",
    "        input_eval = tf.reshape([predicted_id], [1, 1, 1])\n",
    "\n",
    "        text_generated.append(str_voc[predicted_id])\n",
    "\n",
    "    return (start_string + ''.join(text_generated))\n",
    "\n",
    "#default -> should be 10 char long??\n",
    "# start_string = 'There '\n",
    "start_string = 'The little '\n",
    "gen_sen = generate_text(model, start_string, str_voc)\n",
    "gen_sen = gen_sen.split('\\n')[0]\n",
    "\n",
    "print('Starting from \"' + start_string + '\", the generated sentence is:')\n",
    "print('\"' + gen_sen + '\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra Credit: Implement A Sentence Classifier using RNN\n",
    "\n",
    "(Q8) In this OPTIONAL problem, you need to perform sentence classification using RNN. The datasets that we use here is the same dataset that we used in the earlier text generation problem. You will get\n",
    "* 5 additional points if the test accuracy is above 70%\n",
    "* 7 additional points if the test accuracy is above 80%\n",
    "* 9 additional points if the test accuracy is above 85%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating Test and Train Datasets\n",
    "all_sentences, all_labels = load_data('train.csv', with_labels = True)\n",
    "# NOTE: you need to use the same vocabulary to handle your test sentences\n",
    "vocabulary = list(set(\"\".join(all_sentences))) \n",
    "vocabulary.sort()\n",
    "str_voc = \"\".join(vocabulary)\n",
    "train_data = convert_sen_to_data(train_sentences, str_voc)\n",
    "train_mat = tf.keras.preprocessing.sequence.pad_sequences(train_data, maxlen=100, \n",
    "                                                         padding='post', truncating='post',\n",
    "                                                         value=-1)\n",
    "\n",
    "print(f\"The shape of the complete dataset: {train_mat.shape}\")\n",
    "# The training and test examples\n",
    "x_train  = train_mat[:80000]\n",
    "x_test   = train_mat[80000:160000]\n",
    "\n",
    "# The training and test labels\n",
    "y_train  = np.array(all_labels[:80000])\n",
    "y_test   = np.array(all_labels[80000:160000])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example Model\n",
    "# You can update your model here\n",
    "model_c = tf.keras.Sequential([\n",
    "    tf.keras.layers.InputLayer(input_shape=(100,)),\n",
    "    tf.keras.layers.Lambda(lambda x: x+1),\n",
    "    tf.keras.layers.Embedding(\n",
    "        input_dim=voc_size + 1,\n",
    "        output_dim=10,\n",
    "        # Use masking to handle the variable sequence lengths\n",
    "        mask_zero=True),\n",
    "    tf.keras.layers.SimpleRNN(10),\n",
    "    tf.keras.layers.Dense(10, activation='relu'),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "model_c.summary()\n",
    "\n",
    "# Compile and Fit the model\n",
    "model_c.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=False),\n",
    "              optimizer=tf.keras.optimizers.Adam(1e-4),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Training on only 100 datasets: You need to run the model on all the training data for \n",
    "# higher classification accuracy\n",
    "model_c.fit(x=x_train[:100], y = y_train[:100], epochs = 200, batch_size = 32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_c.save(\"rnn_classifier.kmod\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing test accuracy\n",
    "model_cl = tf.keras.models.load_model(\"rnn_classifier.kmod\")\n",
    "y_pred = (model_cl(x_test) > 0.5).numpy().flatten().astype(int)\n",
    "accuracy = np.sum(y_test == y_pred) / y_pred.shape[0]\n",
    "print(f\"Accuracy : {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
